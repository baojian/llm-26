from IPython.display import display, HTML

display(HTML("""
<style>
/* JupyterLab: center notebook and leave margins */
.jp-NotebookPanel-notebook {
  width: 85% !important;              
  margin-left: auto !important;
  margin-right: auto !important;
  max-width: 1100px !important;       /* optional cap */
}

/* Make output area match nicely */
.jp-OutputArea {
  max-width: 100% !important;
}
</style>
"""))
# make high-resolution of images.
%config InlineBackend.figure_format = 'svg'
print("setup jnk layout and figure format.")




















import re
import os
import time
import math

import shutil
import socket
import platform
import requests
import datetime
import tarfile

import spacy
import torch
import numpy as np

from transformers import AutoTokenizer
from transformers import Qwen2Tokenizer, Qwen2TokenizerFast

import tiktoken
from tokenizers import normalizers
from tokenizers.normalizers import NFD, StripAccents
from tokenizers import decoders

from transformers import AutoTokenizer
from tokenizers import Tokenizer
from tokenizers import normalizers
from tokenizers.models import WordPiece
from tokenizers.normalizers import NFD, Lowercase, StripAccents
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.processors import TemplateProcessing

from datasets import load_dataset
import matplotlib.pyplot as plt

jnk_start_time = time.time()








!python -c "import torch; print('torch', torch.__version__); print('cuda?', torch.cuda.is_available()); print('mps?', getattr(torch.backends,'mps',None) is not None and torch.backends.mps.is_available()); print('cuda device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'no gpu'); print('using:', 'cuda' if torch.cuda.is_available() else ('mps' if (getattr(torch.backends,'mps',None) is not None and torch.backends.mps.is_available()) else 'cpu'))"





def detect_torch_device(verbose: bool = True) -> str:
    """
    Returns one of: 'cuda', 'mps', 'cpu'
    Priority: CUDA GPU > Apple MPS > CPU
    """
    has_cuda = torch.cuda.is_available()
    has_mps = getattr(torch.backends, "mps", None) is not None and torch.backends.mps.is_available()

    if has_cuda:
        device = "cuda"
    elif has_mps:
        device = "mps"
    else:
        device = "cpu"

    if verbose:
        print(f"torch: {torch.__version__}")
        print(f"device: {device}")

        if has_cuda:
            print(f"cuda devices: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"  [{i}] {torch.cuda.get_device_name(i)}")
        elif has_mps:
            print("mps available: True (Apple Metal)")
        else:
            print("cpu only")

    return device

device = detect_torch_device()
# generate 2x3 random matrix to check torch
x = torch.rand(2, 3)
x = x.to(device)
print("device:", x.device)





def bytes_to_gb(x: int) -> float:
    return x / (1024 ** 3)

def system_report(path: str = "."):
    print("=== System Report ===")
    print("OS:", platform.system(), platform.release())
    print("Platform:", platform.platform())
    now = datetime.datetime.now().astimezone()
    print("Local time:", now.strftime("%Y-%m-%d %H:%M:%S %Z%z"))
    print("Hostname  :", socket.gethostname())
    print("User      :", os.getenv("USER") or os.getenv("USERNAME") or "unknown")
    
    print("\n=== OS / Python ===")
    print("OS        :", platform.system(), platform.release())
    print("Version   :", platform.version())
    print("Machine   :", platform.machine())
    print("Processor :", platform.processor() or "unknown")
    print("Python    :", platform.python_version())

    # CPU
    print("\n--- CPU ---")
    print("CPU cores (logical):", os.cpu_count())

    # Memory (best-effort, cross-platform)
    print("\n--- Memory (RAM) ---")
    try:
        import psutil  # you already have this in env
        vm = psutil.virtual_memory()
        print(f"Total: {bytes_to_gb(vm.total):.2f} GB")
        print(f"Available: {bytes_to_gb(vm.available):.2f} GB")
        print(f"Used: {bytes_to_gb(vm.used):.2f} GB ({vm.percent}%)")
    except Exception as e:
        print("psutil not available or failed:", e)

    # Disk
    print("\n--- Disk ---")
    total, used, free = shutil.disk_usage(path)
    print("Path checked:", os.path.abspath(path))
    print(f"Total: {bytes_to_gb(total):.2f} GB")
    print(f"Free:  {bytes_to_gb(free):.2f} GB")
    print(f"Used:  {bytes_to_gb(used):.2f} GB")

    # PyTorch device
    print("\n--- PyTorch Device ---")
    try:
        import torch
        cuda = torch.cuda.is_available()
        mps = hasattr(torch.backends, "mps") and torch.backends.mps.is_available()
        print("torch:", torch.__version__)
        print("CUDA available:", cuda)
        print("MPS available:", mps)
        if cuda:
            print("GPU:", torch.cuda.get_device_name(0))
        device = "cuda" if cuda else ("mps" if mps else "cpu")
        print("Suggested device:", device)
    except Exception as e:
        print("torch not available or failed:", e)

system_report(".")














# Make sure bypass proxies for local services (e.g., Ollama on localhost:11434)
os.environ["OLLAMA_HOST"] = "http://127.0.0.1:11434"
os.environ["NO_PROXY"] = "localhost,127.0.0.1"
os.environ["no_proxy"] = "localhost,127.0.0.1"

s = requests.Session()
# IMPORTANT <- ignore ALL proxy env vars
s.trust_env = False  
# make sure ollama is running
print(s.get("http://127.0.0.1:11434/api/version", timeout=3).json())

# !!!! make sure to import after the s.trust_env !!!!

import ollama
from ollama import chat
from ollama import ChatResponse

s = requests.Session()
s.trust_env = False  # <- ignore ALL proxy env vars

response: ChatResponse = chat(model='qwen3:0.6b', messages=[
  {
    'role': 'user',
    'content': 'Fudan University is located in which city? Answer with one word.',
  },
])
print(response['message']['content'])





models = ["qwen3:0.6b", "qwen3:1.7b"]
prompt = "Fudan University is located in which city? Answer with one word."

for model in models:
    print('-' * 50)
    start_time = time.time()
    for _ in range(10):
        resp = ollama.generate(
            model = model,
            prompt = prompt
        )
        print(f"{model} with resp {_ + 1}: {resp["response"]}")
    print(f'total runtime of 10 responses of {model} is: {time.time() - start_time:.1f} seconds')








model = "qwen3:1.7b"
prompt = "I am an undergraduate student, please explain LLMs in three sentences."
resp = ollama.generate(
            model=model,
            prompt=prompt
        )
print(f"Prompt: {prompt} \nResp: {resp["response"]}")








model = "qwen3:0.6b" # "qwen3:1.7b"
prompt = "Fudan University is located in which city? Answer with one word."
num_top_tokens = 20 # number of top alternatives per generated token
resp = ollama.generate(
    model = model,
    prompt = prompt,
    stream = False,
    logprobs = True,
    think = False,
    top_logprobs = num_top_tokens
)
print("response:", repr(resp["response"]))

# Each element usually corresponds to one generated token
for i, lp in enumerate(resp.get("logprobs", [])):
    tok = lp.get("token")
    logp = lp.get("logprob")
    p = math.exp(logp) if logp is not None else None
    print(f"{i:02d} token={tok!r:>16} logp={logp: .4f} p={p:.4f}")





model = "qwen3:0.6b"
prompt = "Fudan University is located in which city? Answer with one word."

res = ollama.generate(
    model=model,
    prompt=prompt,
    logprobs=True,
    think = False,
    top_logprobs=10,
    options={"temperature": 0.0, # greedy decoding, it pick the maximal token
             "num_predict": 20,
            "think": False # do not use thinking model.
            },
)

answer = ''
lp = res["logprobs"]
tokens = [d.get("token", "") for d in lp]
print(f'We use model: {model}')
for i in range(len(lp)):
    tok = lp[i].get("token", "")
    logp = lp[i].get("logprob", None)
    alts = lp[i].get("top_logprobs", [])
    p = math.exp(logp) if logp is not None else float("nan")
    if tok == "\n" or tok == "\n\n": # stop when answer ends (often newline).
        break
    answer += tok
    print(f"--- top probabilities of token-{i:02d} ---")
    for a in alts[:20]:
        prob_a = math.exp(a['logprob'])
        print(f"{a['token']!r:>12}:{prob_a:.5f}")
    print(f"\nPartial Response: {answer}\n")
print(f"Final Response: {answer}")











model = "qwen3:1.7b"
prompt = "Fudan University is located in which city? Answer with one word."

res = ollama.generate(
    model=model,
    prompt=prompt,
    logprobs=True,
    think = False,
    top_logprobs=10,
    options={"temperature": 0.0, # greedy decoding, it pick the maximal token
             "num_predict": 20,
            "think": False # do not use thinking model.
            },
)
print(res["response"])





model = "qwen3:1.7b"
prompt = "Fudan University is located in which city? Answer with one word."
print(f'We use model: {model}')

res = ollama.generate(
    model=model,
    prompt=prompt,
    logprobs=True,
    think = False, # Do not use the thinking model.
    top_logprobs=10,
    options={"temperature": 0.0, # greedy decoding, it pick the maximal token
             "num_predict": 20,
            "think": False # do not use thinking model.
            },
)

answer = ''
lp = res["logprobs"]
tokens = [d.get("token", "") for d in lp]
for i in range(len(lp)):
    tok = lp[i].get("token", "")
    logp = lp[i].get("logprob", None)
    alts = lp[i].get("top_logprobs", [])
    p = math.exp(logp) if logp is not None else float("nan")
    # stop when answer ends (often newline).
    if tok == "\n" or tok == "\n\n": 
        break
    answer += tok
    print(f"--- top probabilities of token-{i:02d} ---")
    for a in alts[:5]:
        prob_a = math.exp(a['logprob'])
        print(f"{a['token']!r:>5}:{prob_a:.5f}", end="")
    print(f"\nPartial Response: {answer}\n")
print(f"Final Response: {answer}")








# packages needed in this section
import re
import os
import spacy
import numpy as np
import matplotlib.pyplot as plt








# Task: Find woodchuck or Woodchuck : Disjunction

test_str = "This string contains Woodchuck and woodchuck."

result=re.search(pattern="[wW]oodchuck", string=test_str)
print("Matched" if result is not None else "Not found")
result=re.search(pattern=r"[wW]ooodchuck", string=test_str)
print("Matched" if result is not None else "Not found")


# Find the word "woodchuck" in the following test string

test_str = "interesting links to woodchucks ! and lemurs!"
result = re.search(pattern="woodchuck", string=test_str)
print("Matched" if result is not None else "Not found")


# Find !, it follows the same way:

test_str = "interesting links to woodchucks ! and lemurs!"
result = re.search(pattern="!", string=test_str)
print("Matched" if result is not None else "Not found")
result = re.search(pattern="!!", string=test_str)
print("Matched" if result is not None else "Not found")
assert re.search(pattern="!!", string=test_str) == None # match nothing





# Find any single digit in a string.

result=re.search(pattern=r"[0123456789]", string="plenty of 7 to 5")
print("Matched" if result is not None else "Not found", result)
result=re.search(pattern=r"[0-9]", string="plenty of 7 to 5")
print("Matched" if result is not None else "Not found", result)





# Negation: If the caret ^ is the first symbol after [,
# the resulting pattern is negated. For example, the pattern 
# [^a] matches any single character (including special characters) except a.

# -- not an upper case letter
print(re.search(pattern=r"[^A-Z]", string="Oyfn pripetchik"))
# -- neither 'S' nor 's'
print(re.search(pattern=r"[^Ss]", string="I have no exquisite reason for't"))
# -- not a period
print(re.search(pattern=r"[^.]", string="our resident Djinn"))
# -- either 'e' or '^'
print(re.search(pattern=r"[e^]", string="look up ^ now"))
# -- the pattern â€˜a^bâ€™
print(re.search(pattern=r'a\^b', string=r'look up a^b now'))





# More disjuncations with or or operation
str1 = "Woodchucks is another name for groundhog!"
result = re.search(pattern="groundhog|woodchuck",string=str1)
print(result)


str1 = "Find all woodchuckk Woodchuck Groundhog groundhogxxx!"
result = re.findall(pattern="[gG]roundhog|[Ww]oodchuck",string=str1)
print(result)





# Some special chars

# ?: Optional previous char
str1 = "Find all color colour colouur colouuur colouyr"
result = re.findall(pattern="colou?r",string=str1)
print(result)

# *: 0 or more of previous char
str1 = "Find all color colour colouur colouuur colouyr"
result = re.findall(pattern="colou*r",string=str1)
print(result)

# +: 1 or more of previous char
str1 = "baa baaa baaaa baaaaa"
result = re.findall(pattern="baa+",string=str1)
print(result)
# .: any char
str1 = "begin begun begun beg3n"
result = re.findall(pattern="beg.n",string=str1)
print(result)
str1 = "The end."
result = re.findall(pattern=r"\.$",string=str1)
print(result)
str1 = "The end? The end. #t"
result = re.findall(pattern=r".$",string=str1)
print(result)


# find all "the" in a raw text use above operations.

text = "If two sequences in an alignment share a common ancestor, \
mismatches can be interpreted as point mutations and gaps as indels (that \
is, insertion or deletion mutations) introduced in one or both lineages in \
the time since they diverged from one another. In sequence alignments of \
proteins, the degree of similarity between amino acids occupying a \
particular position in the sequence can be interpreted as a rough \
measure of how conserved a particular region or sequence motif is \
among lineages. The absence of substitutions, or the presence of \
only very conservative substitutions (that is, the substitution of \
amino acids whose side chains have similar biochemical properties) in \
a particular region of the sequence, suggest [3] that this region has \
structural or functional importance. Although DNA and RNA nucleotide bases \
are more similar to each other than are amino acids, the conservation of \
base pairs can indicate a similar functional or structural role."
matches = re.findall("[^a-zA-Z][tT]he[^a-zA-Z]", text)
print(matches)


# Above has spaces before or after each word. A nicer way is to do the following
matches = re.findall(r"\b[tT]he\b", text)
print(matches)





word_re = re.compile(r"[A-Za-z]+(?:'[A-Za-z]+)?")

text = """SenjÅ no Valkyria 3 : Unrecorded Chronicles ( Japanese : æˆ¦å ´ã®ãƒ´ã‚¡ãƒ«ã‚­ãƒ¥ãƒªã‚¢3 ,\
lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles \
III outside Japan , is a tactical role @-@ playing video game developed by Sega and \
Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , \
it is the third game in the Valkyria series . Employing the same fusion of tactical \
and real @-@ time gameplay as its predecessors"""

tokens = word_re.findall(text)
print(len(tokens))
print(tokens)








# Use split method via the whitespace " "
text = """While the Unix command sequence just removed all the numbers and punctuation"""
print(text.split(" "))


# But, we have punctuations, icons, and many other small issues.

text = """Don't you love ğŸ¤— Transformers? We sure do."""
print(text.split(" "))





text = 'å§šæ˜è¿›å…¥æ€»å†³èµ›'
print(text.split(" "))





from spacy.lang.zh import Chinese
nlp = spacy.load("zh_core_web_sm")

text = 'å§šæ˜è¿›å…¥æ€»å†³èµ›'
doc = nlp(text)
print([token for token in doc])

# ? can be successfully split from Transformers
text = """Don't you love ğŸ¤— Transformers? We sure do."""
doc = nlp(text)
print([token for token in doc]) 

# Chinese Character level tokenization
nlp_ch = Chinese()
text = 'å§šæ˜è¿›å…¥æ€»å†³èµ›'
print([*nlp_ch(text)])





import spacy
from spacy.tokens import Doc

nlp = spacy.blank("en")

text = "Hello, world!"
chars = [c for c in text]
doc = Doc(nlp.vocab, words=chars)

print([t.text for t in doc])

# ignore white space
chars = [c for c in text if not c.isspace()]
doc = Doc(nlp.vocab, words=chars)

print([t.text for t in doc])





text = """Special characters and numbers will need to be kept in prices ($45.55) and dates (01/02/06); 
we donâ€™t want to segment that price into separate tokens of â€œ45â€ and â€œ55â€. And there are URLs (https://www.stanford.edu),
Twitter hashtags (#nlproc), or email addresses (someone@cs.colorado.edu)."""
text = text.replace("\n", " ").strip()
nlp = spacy.load('en_core_web_sm')
doc = nlp(text)
all_tokens = [token for token in doc]
print(all_tokens)





text = """è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆè‹±è¯­ï¼šNatural Language Processingï¼Œç¼©å†™ä½œ NLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦é¢†åŸŸçš„äº¤å‰å­¦ç§‘ï¼Œ\
ç ”ç©¶è®¡ç®—æœºå¤„ç†ã€ç†è§£ä¸ç”Ÿæˆäººç±»è¯­è¨€çš„æŠ€æœ¯ã€‚æ­¤é¢†åŸŸæ¢è®¨å¦‚ä½•å¤„ç†åŠè¿ç”¨è‡ªç„¶è¯­è¨€ï¼›è‡ªç„¶è¯­è¨€å¤„ç†åŒ…æ‹¬å¤šæ–¹é¢å’Œæ­¥éª¤ï¼Œ\
åŸºæœ¬æœ‰è®¤çŸ¥ã€ç†è§£ã€ç”Ÿæˆç­‰éƒ¨åˆ†ã€‚è‡ªç„¶è¯­è¨€è®¤çŸ¥å’Œç†è§£æ˜¯è®©ç”µè„‘æŠŠè¾“å…¥çš„è¯­è¨€å˜æˆç»“æ„åŒ–ç¬¦å·ä¸è¯­ä¹‰å…³ç³»ï¼Œ\
ç„¶åæ ¹æ®ç›®çš„å†å¤„ç†ã€‚è‡ªç„¶è¯­è¨€ç”Ÿæˆç³»ç»Ÿåˆ™æ˜¯æŠŠè®¡ç®—æœºæ•°æ®è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ã€‚\
è‡ªç„¶è¯­è¨€å¤„ç†è¦ç ”åˆ¶è¡¨ç¤ºè¯­è¨€èƒ½åŠ›å’Œè¯­è¨€åº”ç”¨çš„æ¨¡å‹, å»ºç«‹è®¡ç®—æ¡†æ¶æ¥å®ç°å¹¶å®Œå–„è¯­è¨€æ¨¡å‹ï¼Œ\
å¹¶æ ¹æ®è¯­è¨€æ¨¡å‹è®¾è®¡å„ç§å®ç”¨ç³»ç»ŸåŠæ¢è®¨è¿™äº›ç³»ç»Ÿçš„è¯„æµ‹æŠ€æœ¯ã€‚â€Œâ€Œ"""
nlp = spacy.load("zh_core_web_sm")
doc = nlp(text)
print([sent.text for sent in doc.sents])





 # You need to install it via: python -m spacy download zh_core_web_sm
from spacy.lang.zh.examples import sentences 
nlp = spacy.load("zh_core_web_sm")
doc = nlp(sentences[0])
text = """\
å­—èŠ‚å¯¹ç¼–ç æ˜¯ä¸€ç§ç®€å•çš„æ•°æ®å‹ç¼©å½¢å¼ï¼Œè¿™ç§æ–¹æ³•ç”¨æ•°æ®ä¸­ä¸å­˜çš„ä¸€ä¸ªå­—èŠ‚è¡¨ç¤ºæœ€å¸¸å‡ºç°çš„è¿ç»­å­—èŠ‚æ•°æ®ã€‚\
è¿™æ ·çš„æ›¿æ¢éœ€è¦é‡å»ºå…¨éƒ¨åŸå§‹æ•°æ®ã€‚å­—èŠ‚å¯¹ç¼–ç å®ä¾‹: å‡è®¾æˆ‘ä»¬è¦ç¼–ç æ•°æ® aaabdaaabac, å­—èŠ‚å¯¹â€œaaâ€ \
å‡ºç°æ¬¡æ•°æœ€å¤šï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨æ•°æ®ä¸­æ²¡æœ‰å‡ºç°çš„å­—èŠ‚â€œZâ€æ›¿æ¢â€œaaâ€å¾—åˆ°æ›¿æ¢è¡¨Z <- aa æ•°æ®è½¬å˜ä¸º ZabdZabac. \
åœ¨è¿™ä¸ªæ•°æ®ä¸­ï¼Œå­—èŠ‚å¯¹â€œZaâ€å‡ºç°çš„æ¬¡æ•°æœ€å¤šï¼Œæˆ‘ä»¬ç”¨å¦å¤–ä¸€ä¸ªå­—èŠ‚â€œYâ€æ¥æ›¿æ¢å®ƒï¼ˆè¿™ç§æƒ…å†µä¸‹ç”±äºæ‰€æœ‰çš„â€œZâ€éƒ½å°†è¢«æ›¿æ¢ï¼Œ\
æ‰€ä»¥ä¹Ÿå¯ä»¥ç”¨â€œZâ€æ¥æ›¿æ¢â€œZaâ€ï¼‰ï¼Œå¾—åˆ°æ›¿æ¢è¡¨ä»¥åŠæ•°æ® Z <- aa, Y <- Za, YbdYbac.
"""
doc = nlp(text)
sentences = [sent.text for sent in doc.sents]
print(sentences)


# A modern and fast NLP library that includes support for sentence segmentation. 
# spaCy uses a statistical model to predict sentence boundaries, which can be more accurate 
# than rule-based approaches for complex texts.

nlp = spacy.load("en_core_web_sm")
doc = nlp("Here is a sentence. Here is another one! And the last one.")
sentences = [sent.text for sent in doc.sents]
for ind, sent in enumerate(sentences):
    print(f"sentence-{ind}: {sent}\n")








import spacy
import pandas as pd

prompt = "Fudan University is located in Shanghai. are Universities"
nlp = spacy.load("en_core_web_sm")
doc = nlp(prompt)

df = pd.DataFrame([{
    "text": t.text,
    "lemma": t.lemma_, # Lemma: The base form of the word.
    "pos": t.pos_, # POS: The simple UPOS part-of-speech tag.
    "tag": t.tag_, # Tag: The detailed part-of-speech tag.
    "dep": t.dep_, # Dep: Syntactic dependency, i.e. the relation between tokens.
    "shape": t.shape_, # Shape: The word shape â€“ capitalization, punctuation, digits.
    "is_alpha": t.is_alpha, # is alpha: Is the token an alpha character?
    "is_stop": t.is_stop, # is stop: Is the token part of a stop list.
} for t in doc])
df

















os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_ETAG_TIMEOUT"] = "60"
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "60"

from datasets import load_dataset
# loads train/validation/test
wiki2_ds = load_dataset("wikitext", "wikitext-2-raw-v1")
wiki2_train = wiki2_ds["train"]


summary = pd.DataFrame([
    {
        "split": name,
        "num_rows": d.num_rows,
        "num_columns": d.num_columns,
        "columns": ", ".join(d.column_names),
    }
    for name, d in wiki2_ds.items()
])
summary


preview = []
for name, d in wiki2_ds.items():
    for i in range(5):
        preview.append({"split": name, "idx": i, **d[i]})

pd.DataFrame(preview)





import re
import numpy as np
import matplotlib.pyplot as plt

start_time = time.time()
wiki2_train = wiki2_ds["train"]
word_re = re.compile(r"[A-Za-z]+(?:'[A-Za-z]+)?") # simple word tokenizer

def heaps_curve(dataset, step=1000):
    V = set()
    N = 0
    Ns, Vs = [], []
    for ex in dataset:
        text = ex["text"]
        if not text or text.strip() == "": # empty word -> continue
            continue
        words = word_re.findall(text.lower()) # make all words to low cases
        for w in words:
            N += 1
            V.add(w)
            if N % step == 0:
                Ns.append(N)
                Vs.append(len(V))
    return np.array(Ns), np.array(Vs)

Ns, Vs = heaps_curve(wiki2_train, step=1000)

# Fit log |V| = log k + beta log N  -> linear regression on logs
logN = np.log(Ns)
logV = np.log(Vs)
beta, logk = np.polyfit(logN, logV, 1)
k = np.exp(logk)

print(f"Fitted Heaps' law: |V| â‰ˆ {k:.2f} * T^{beta:.3f}")
print(f"Total runtime: {time.time() - start_time:.3f} seconds")

# Plot (log-log)
plt.figure(figsize=(7, 5))
plt.loglog(Ns, Vs, marker='o', linestyle='none', markersize=5, label="Empirical (Wikitext-2)")

# fitted line
Ns_line = np.linspace(Ns[0], Ns[-1], 200)
Vs_line = k * (Ns_line ** beta)
plt.loglog(Ns_line, Vs_line, label=fr"Fitted ($k={k:.3f},\beta=${beta:.3f})")

plt.xlabel("$T$ (tokens)", fontsize = 15)
plt.ylabel("$|V|$ (vocab)", fontsize = 15)
plt.title(r"Heaps' Law (Simple tokenization): $|V|=k \cdot T^{\beta}$", fontsize = 15)
plt.legend(fontsize = 15)
plt.tight_layout()
plt.show()





start_time = time.time()

nlp = spacy.load(
    "en_core_web_sm", 
    disable=["tagger","parser","ner","lemmatizer"]
)
wiki2_train = wiki2_ds["train"]

def heaps_curve_spacy(dataset, step=10_000, batch_size=256):
    V = set()
    N = 0
    Ns, Vs = [], []

    texts = (ex["text"] for ex in dataset if ex["text"] and ex["text"].strip())
    for doc in nlp.pipe(texts, batch_size=batch_size):
        for tok in doc:
            if tok.is_alpha: # choose your definition of "word"
                w = tok.text.lower()
                N += 1
                V.add(w)
                if N % step == 0:
                    Ns.append(N)
                    Vs.append(len(V))
    return np.array(Ns), np.array(Vs)

Ns, Vs = heaps_curve_spacy(wiki2_train, step=1000)

# Fit log |V| = log k + beta log N
logN = np.log(Ns)
logV = np.log(Vs)
beta, logk = np.polyfit(logN, logV, 1)
k = np.exp(logk)

print(f"Fitted Heaps' law (spaCy): |V| â‰ˆ {k:.2f} * T^{beta:.3f}")
print(f"Total runtime: {time.time() - start_time:.3f} seconds")
# Plot (log-log)
plt.figure(figsize=(7, 5))
plt.loglog(Ns, Vs, marker='o', linestyle='none', markersize=5, label="Empirical (Wikitext-2)")

# fitted line
Ns_line = np.linspace(Ns[0], Ns[-1], 200)
Vs_line = k * (Ns_line ** beta)
plt.loglog(Ns_line, Vs_line, label=fr"Fitted ($k={k:.3f},\beta=${beta:.3f})")

plt.xlabel("$T$ (tokens)", fontsize = 15)
plt.ylabel("$|V|$ (vocab)", fontsize = 15)
plt.title(r"Heaps' Law (Spacy tokenization): $|V|=k\cdot T^{\beta}$", fontsize = 15)
plt.legend(fontsize = 15)
plt.tight_layout()
plt.show()











from datasets import load_dataset
wiki103_ds = load_dataset("wikitext", "wikitext-103-raw-v1")  # will hit cache if present


summary = pd.DataFrame([
    {
        "split": name,
        "num_rows": d.num_rows,
        "num_columns": d.num_columns,
        "columns": ", ".join(d.column_names),
    }
    for name, d in wiki103_ds.items()
])
summary


start_time = time.time()
wiki103_train = wiki103_ds["train"]
word_re = re.compile(r"[A-Za-z]+(?:'[A-Za-z]+)?")

def heaps_curve(dataset, step=1000):
    V = set()
    N = 0
    Ns, Vs = [], []
    for ex in dataset:
        text = ex["text"]
        if not text or text.strip() == "": # empty word -> continue
            continue
        words = word_re.findall(text.lower()) # make all words to low cases
        for w in words:
            N += 1
            V.add(w)
            if N % step == 0:
                Ns.append(N)
                Vs.append(len(V))
    return np.array(Ns), np.array(Vs)

Ns, Vs = heaps_curve(wiki103_train, step=1000)

# Fit log |V| = log k + beta log N  -> linear regression on logs
logN = np.log(Ns)
logV = np.log(Vs)
beta, logk = np.polyfit(logN, logV, 1)
k = np.exp(logk)

print(f"Fitted Heaps' law (spaCy): |V| â‰ˆ {k:.2f} * T^{beta:.3f}")
print(f"Total runtime: {time.time() - start_time:.3f} seconds")
# Plot (log-log)
plt.figure(figsize=(7, 5))
plt.loglog(Ns, Vs, marker='o', linestyle='none', markersize=5, label="Empirical (Wikitext-103)")

# fitted line
Ns_line = np.linspace(Ns[0], Ns[-1], 200)
Vs_line = k * (Ns_line ** beta)
plt.loglog(Ns_line, Vs_line, label=fr"Fitted ($k={k:.3f},\beta=${beta:.3f})")

plt.xlabel("$N$ (tokens)", fontsize = 15)
plt.ylabel("$|V|$ (vocab)", fontsize = 15)
plt.title(r"Heaps' Law (Spacy tokenization): $|V|=k \cdot T^{\beta}$", fontsize = 15)
plt.legend(fontsize = 15)
plt.tight_layout()
plt.show()








import time
import tarfile

path = "../.datasets/books1.tar.gz"
start_time = time.time()
with tarfile.open(path, "r:gz") as tar:
    names = tar.getnames()
    print("num members:", len(names))
    print("\n".join(names[:20]))
print(f"total time to scan BookCorpus: {time.time() - start_time:.2f} seconds")








import json
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker

def load_heaps_jsonl(log_path: str):
    Ns, Vs = [], []
    with open(log_path, "r", encoding="utf-8") as f:
        for line in f:
            r = json.loads(line)
            Ns.append(r["total_tokens"])
            Vs.append(r["vocab_size"])
    return np.array(Ns), np.array(Vs)

Ns, Vs = load_heaps_jsonl("assets/books1_wordcounts.heaps_log.jsonl")

# fit log |V| = log k + beta log N
beta, logk = np.polyfit(np.log(Ns), np.log(Vs), 1)
k = np.exp(logk)
print(f"Fit: |V| â‰ˆ {k:.2f} * T^{beta:.3f}")


plt.figure(figsize=(7,5))

plt.loglog(Ns, Vs, marker="o", linestyle="none", 
           markersize=5, label="Empirical (BookCorpus)")
Ns_line = np.linspace(Ns[0], Ns[-1], 200)
plt.loglog(Ns_line, k * (Ns_line ** beta), label=fr"Fitted Curve $(k={k:.3f},\beta=${beta:.3f})")
plt.xlabel("$T$ (tokens)", fontsize = 14)
plt.ylabel("$|V|$ (vocab)", fontsize = 14)
plt.title(r"Heaps' Law (Simple tokenization): $|V|=k \cdot T^{\beta}$", fontsize = 14)
plt.legend(fontsize = 14)
plt.tight_layout()
plt.show()





import tiktoken











# Load cl100k_tokenizer (it was used by gpt-3.5-turbo family, gpt-4, and also gpt-4-turbo

cl100k_tokenizer = tiktoken.get_encoding("cl100k_base")

text = "Don't you love ğŸ¤— Transformers? We sure do."
token_ids = cl100k_tokenizer.encode(text)
token_strs = [cl100k_tokenizer.decode([tid]) for tid in token_ids]

print("ids:", token_ids)
print("tokens:", token_strs)





gpt35_tokenizer = tiktoken.get_encoding("o200k_base")

token_ids = gpt35_tokenizer.encode(text)
token_strs = [gpt35_tokenizer.decode([tid]) for tid in token_ids]

print("ids:", token_ids)
print("tokens:", token_strs)

# Count tokens by counting the length of the list returned by .encode().
def num_tokens_from_string(string: str, encoding_name: str) -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens
print("---")

text = "Don't you love ğŸ¤— Transformers? We sure do."
print(f'\"{text}\" has been encoded into {num_tokens_from_string(text, "cl100k_base")} subwords')
print(f'\"{text}\" has been encoded into {num_tokens_from_string(text, "o200k_base")} subwords')





# .decode() converts a list of token integers to a string.
encode_ids = [83, 1609, 5963, 374, 2294, 0, 11, 2025]
print(f'the decoded string is: \"{gpt35_tokenizer.decode(encode_ids)}\"')





text = """
Chapters 5 to 8 teach the basics of ğŸ¤— Datasets and ğŸ¤— Tokenizers before diving into classic NLP tasks.\
By the end of this part, you will be able to tackle the most common NLP problems by yourself. \
By the end of this part, you will be ready to apply ğŸ¤— Transformers to (almost) any machine \
learning problem! E=mc^2. f(x) = x^2+y^2, print('hello world!â€™) baojianzhou. asdasfasdgasdg
"""
token_ids = gpt35_tokenizer.encode(text)
token_strs = [gpt35_tokenizer.decode([tid]) for tid in token_ids]
print(token_ids)
print(token_strs)





text = """acetylsalicylic acid, aspirin, Group of substances: organic Physical appearance: \
colorless needles crystals Empirical formula (Hill's system for organic substances): \
C9H8O4 Structural formula as text: CH3COOC6H4COOH, 
the smiles format of it is CC(=O)OC1=CC=CC=C1C(O)=O."""

token_ids = gpt35_tokenizer.encode(text)
token_strs = [gpt35_tokenizer.decode([tid]) for tid in token_ids]
print(token_ids)
print(token_strs) # CC(=O)OC1=CC=CC=C1C(O)=O. should be a whole, but it is tokenized into small pieces.











from datasets import load_dataset
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

# load wikitext-103
wiki103_ds = load_dataset("wikitext", "wikitext-103-raw-v1")  # train/validation/test

# Step 1: pre-training
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = Whitespace()





start_time = time.time()

# build tokenizer + trainer
trainer = BpeTrainer(
    vocab_size=30000,
    special_tokens=["[UNK]", "[PAD]", "[BOS]", "[EOS]", "[MASK]", "[CLS]", "[SEP]"]
)
# iterator over ALL splits
def batch_iterator(batch_size=1000):
    for split in ["train", "validation", "test"]:
        for i in range(0, len(wiki103_ds[split]), batch_size):
            yield ds[split][i:i+batch_size]["text"]
tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)
# It will take above 1 second
print(f"Total runtime of training tokenizer on wikitext-103: {time.time() - start_time:.3f} seconds")





# Step 3: save the trained tokenizer
tokenizer.save("assets/wikitext103-bpe.json")


tokenizer = Tokenizer.from_file("assets/wikitext103-bpe.json")
output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
print(output.tokens)
print(output.ids)








# Step 1: normalization, Normalization is, in a nutshell, a set of operations you apply to 
# a raw string to make it less random or â€œcleanerâ€. Common operations include stripping 
# whitespace, removing accented characters or lowercasing all text. 

from tokenizers import normalizers
from tokenizers.normalizers import NFD, StripAccents
normalizer = normalizers.Sequence([NFD(), StripAccents()])

print(normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
tokenizer.normalizer = normalizer





# Step 2: Pre-Tokenization, Pre-tokenization is the act of splitting a text into smaller 
# objects that give an upper bound to what your tokens will be at the end of training. 
# A good way to think of this is that the pre-tokenizer will split your text into â€œwordsâ€
# and then, your final tokens will be parts of those words. An easy way to pre-tokenize 
# inputs is to split on spaces and punctuations, which is done by the pre_tokenizers.
from tokenizers.pre_tokenizers import Whitespace
pre_tokenizer = Whitespace()
pre_tokenizer.pre_tokenize_str("Hello! How are you? I'm fine, thank you.")


from tokenizers import pre_tokenizers
from tokenizers.pre_tokenizers import Digits
pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])
pre_tokenizer.pre_tokenize_str("Call 911!")











from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[("[CLS]", 1), ("[SEP]", 2)],
)


# BERT tokenization building

start_time = time.time()

from transformers import AutoTokenizer
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
bert_tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))

from tokenizers import normalizers
from tokenizers.normalizers import NFD, Lowercase, StripAccents
bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])

from tokenizers.pre_tokenizers import Whitespace
bert_tokenizer.pre_tokenizer = Whitespace()

from tokenizers.processors import TemplateProcessing
bert_tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", 1),
        ("[SEP]", 2),
    ],
)
from tokenizers.trainers import WordPieceTrainer
trainer = WordPieceTrainer(vocab_size=30522, special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

# iterator over ALL splits
def batch_iterator(batch_size=1000):
    for split in ["train", "validation", "test"]:
        for i in range(0, len(wiki103_ds[split]), batch_size):
            yield ds[split][i:i+batch_size]["text"]
bert_tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)
# It will take above 1 second
print(f"Total runtime of training tokenizer on wikitext-103: {time.time() - start_time:.3f} seconds")
bert_tokenizer.save("assets/wikitext103-bert.json")





output = bert_tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
print(output.ids)
# [1, 5993, 919, 16, 67, 11, 1772, 5, 1972, 1700, 2358, 0, 35, 2]
bert_tokenizer.decode([1, 5993, 919, 16, 67, 11, 1772, 5, 1972, 1700, 2358, 0, 35, 2])
# "Hello , y ' all ! How are you ?"


output = bert_tokenizer.encode("Welcome to the ğŸ¤— Tokenizers library.")
print(output.tokens)
# ["[CLS]", "welcome", "to", "the", "[UNK]", "tok", "##eni", "##zer", "##s", "library", ".", "[SEP]"]
bert_tokenizer.decode(output.ids)
# "welcome to the tok ##eni ##zer ##s library ."











from transformers import Qwen2Tokenizer, Qwen2TokenizerFast


tokenizer = Qwen2TokenizerFast.from_pretrained(pretrained_model_name_or_path="Qwen/Qwen-tokenizer")
text = "What is natural language processing? ğŸ¤—"
encoded = tokenizer(text, return_tensors="pt")
print("Input IDs:", encoded["input_ids"])


from transformers import AutoTokenizer
# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-V3")
# Example: tokenize some text
text = "What is natural language processing? ğŸ¤—"
encoded = tokenizer(text, return_tensors="pt")
print("Input IDs:", encoded["input_ids"])
decoded = tokenizer.decode(encoded["input_ids"][0])
print("Decoded text:", decoded)
decoded = tokenizer.decode(encoded["input_ids"][0], skip_special_tokens=True)
print("Decoded with special text:", decoded)


vocab = tokenizer.get_vocab()
print("Vocab size (attribute):", tokenizer.vocab_size)
print("Vocab size (dict length):", len(vocab))
id_to_token = {id_: tok for tok, id_ in vocab.items()}
pairs = [[i, repr(id_to_token[i])]for i in range(20)] # show first 20 tokens
print(pairs)


from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-72B-Instruct")
text = "What is natural language processing? ğŸ¤—"
encoded = tokenizer(text, return_tensors="pt")
print("Input IDs:", encoded["input_ids"])
# Decode including special tokens
decoded = tokenizer.decode(encoded["input_ids"][0])
print("Decoded text:", decoded)


from transformers import AutoTokenizer
# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")
# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
# tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/")
# Download vocabulary from huggingface.co and define model-specific arguments
tokenizer = AutoTokenizer.from_pretrained("FacebookAI/roberta-base", add_prefix_space=True)





































# define minimum edit distance algorithm via dynamic programming
def minimum_edit_distance(source, target):
    n = len(source)
    m = len(target)
    d_mat = np.zeros((n + 1, m + 1))
    for i in range(1, n + 1):
        d_mat[i, 0] = i
    for j in range(1, m + 1):
        d_mat[0, j] = j
    for i in range(1, n + 1):
        for j in range(1, m + 1):
            sub = 0 if source[i - 1] == target[j - 1] else 2
            del_ = d_mat[i - 1][j] + 1
            ins_ = d_mat[i][j - 1] + 1
            d_mat[i][j] = min(del_, ins_, d_mat[i - 1][j - 1] + sub)
    trace, align_source, align_target = backtrack_alignment(source, target, d_mat)
    return d_mat[n, m], trace, align_source, align_target

def backtrack_alignment(source, target, d_mat):
    align_source, align_target = [], []
    i, j = len(source), len(target)
    back_trace = [[i, j]]

    while (i, j) != (0, 0):
        # boundary cases first (avoid negative indexing)
        if i == 0:
            back_trace.append([i, j - 1])
            align_source = ["*"] + align_source
            align_target = [target[j - 1]] + align_target
            j -= 1
            continue
        if j == 0:
            back_trace.append([i - 1, j])
            align_source = [source[i - 1]] + align_source
            align_target = ["*"] + align_target
            i -= 1
            continue

        sub_cost = 0 if source[i - 1] == target[j - 1] else 2

        # prefer substitution/match when optimal (your tie-break rule)
        if d_mat[i, j] == d_mat[i - 1, j - 1] + sub_cost:
            back_trace.append([i - 1, j - 1])
            align_source = [source[i - 1]] + align_source
            align_target = [target[j - 1]] + align_target
            i, j = i - 1, j - 1

        # deletion
        elif d_mat[i, j] == d_mat[i - 1, j] + 1:
            back_trace.append([i - 1, j])
            align_source = [source[i - 1]] + align_source
            align_target = ["*"] + align_target
            i -= 1

        # insertion
        else:
            back_trace.append([i, j - 1])
            align_source = ["*"] + align_source
            align_target = [target[j - 1]] + align_target
            j -= 1

    return back_trace, align_source, align_target

# test the minimum edit distance
def test_med(source, target):
    med, trace, align_source, align_target = minimum_edit_distance(source, target)
    print(f"input source: {source} and target: {target}")
    print(f"med: {med}")
    print(f"trace: {trace}")
    print(f"aligned source: {align_source}")
    print(f"aligned target: {align_target}")


test_med(source="INTENTION", target="EXECUTION")


test_med(source="AGGCTATCACCTGACCTCCAGGCCGATGCCC", target="TAGCTATCACGACCGCGGTCGATTTGCCCGAC")





# Finally, you all done! Please submit your jupyter notebook via elearning!

your_chinese_name = "XYZ" # put your chinese name here
your_student_id = "" # put your student id here

jnk_end_time = time.time()
total = jnk_end_time - jnk_start_time
h, rem = divmod(int(total), 3600)
m, s = divmod(rem, 60)

print(f"Total runtime: {h} h {m} min {s} s")
print(f"I am {your_chinese_name}:{your_student_id}, I am doing a great job!")





!jupyter nbconvert lecture-01-exercise-tokenization.ipynb --to html --template classic --embed-images






