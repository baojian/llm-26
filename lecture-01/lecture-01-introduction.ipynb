{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8c6cb6-e001-4bda-b191-0b6b727340fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "/* JupyterLab: center notebook and leave margins */\n",
       ".jp-NotebookPanel-notebook {\n",
       "  width: 85% !important;              \n",
       "  margin-left: auto !important;\n",
       "  margin-right: auto !important;\n",
       "  max-width: 1100px !important;       /* optional cap */\n",
       "}\n",
       "\n",
       "/* Make output area match nicely */\n",
       ".jp-OutputArea {\n",
       "  max-width: 100% !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "/* JupyterLab: center notebook and leave margins */\n",
    ".jp-NotebookPanel-notebook {\n",
    "  width: 85% !important;              \n",
    "  margin-left: auto !important;\n",
    "  margin-right: auto !important;\n",
    "  max-width: 1100px !important;       /* optional cap */\n",
    "}\n",
    "\n",
    "/* Make output area match nicely */\n",
    ".jp-OutputArea {\n",
    "  max-width: 100% !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e4a5c6-5554-4f5a-942b-538abdd19fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1\n",
      "device: mps\n",
      "mps available: True (Apple Metal)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def detect_torch_device(verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Returns one of: 'cuda', 'mps', 'cpu'\n",
    "    Priority: CUDA GPU > Apple MPS > CPU\n",
    "    \"\"\"\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    has_mps = getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available()\n",
    "\n",
    "    if has_cuda:\n",
    "        device = \"cuda\"\n",
    "    elif has_mps:\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"torch: {torch.__version__}\")\n",
    "        print(f\"device: {device}\")\n",
    "\n",
    "        if has_cuda:\n",
    "            print(f\"cuda devices: {torch.cuda.device_count()}\")\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                print(f\"  [{i}] {torch.cuda.get_device_name(i)}\")\n",
    "        elif has_mps:\n",
    "            print(\"mps available: True (Apple Metal)\")\n",
    "        else:\n",
    "            print(\"cpu only\")\n",
    "\n",
    "    return device\n",
    "\n",
    "device = detect_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29585c2-4b37-4651-ae84-d9eecc56a7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1\n",
      "mps built: True\n",
      "mps available: True\n",
      "tensor([[0.2233, 0.3450, 0.9225],\n",
      "        [0.7543, 0.0805, 0.5617]])\n",
      "device: mps:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"mps built:\", torch.backends.mps.is_built())\n",
    "print(\"mps available:\", torch.backends.mps.is_available())\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(x)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    x = x.to(\"mps\")\n",
    "    print(\"device:\", x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001a351-3ccb-4a1b-85d7-fb4db4e22a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08c006fe-ca20-452d-8263-e23a2bf29d1e",
   "metadata": {},
   "source": [
    "## 1. Explore LLMs via ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d170128b-66ac-4b06-8671-0ce878a617ea",
   "metadata": {},
   "source": [
    "### 1.1  Install ollama and download Qwen3:0.6b and Qwen3:1.7b\n",
    "\n",
    "We can download some popular LLMs such as Qwen series.\n",
    "\n",
    "- Download ollama to your laptop from [https://ollama.com/download](https://ollama.com/download)\n",
    "- Run ```shell ollama run qwen3:0.6b ```, it will automatically download the model into your local disk. It takes about 498MB disk space.\n",
    "- Similarly, you can run, ```shell ollama run qwen3:1.7b ```. It takes about 1.3GB disk space.\n",
    "\n",
    "You can install the python api of ollama via the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110bf6b-064d-4c74-9f96-94e4fc702b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::ollama -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756eae67-2082-4131-bc2b-17b95b04bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge ollama-python -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cfbce0-e58e-4558-8291-4536201feebb",
   "metadata": {},
   "source": [
    "### 1.2 Get response for a given prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b1791de-ae7b-40bf-9c38-e175a0527044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "qwen3:0.6b with resp 1: Hangzhou\n",
      "qwen3:0.6b with resp 2: Shanghai\n",
      "qwen3:0.6b with resp 3: Beijing\n",
      "qwen3:0.6b with resp 4: Shanghai\n",
      "qwen3:0.6b with resp 5: Beijing\n",
      "qwen3:0.6b with resp 6: Hangzhou\n",
      "qwen3:0.6b with resp 7: Hangzhou\n",
      "qwen3:0.6b with resp 8: Hangzhou\n",
      "qwen3:0.6b with resp 9: Hangzhou\n",
      "qwen3:0.6b with resp 10: Shanghai\n",
      "total runtime of 10 responses of qwen3:0.6b is: 9.5 seconds\n",
      "--------------------------------------------------\n",
      "qwen3:1.7b with resp 1: Shanghai\n",
      "qwen3:1.7b with resp 2: Shanghai\n",
      "qwen3:1.7b with resp 3: Shanghai\n",
      "qwen3:1.7b with resp 4: Shanghai\n",
      "qwen3:1.7b with resp 5: Shanghai\n",
      "qwen3:1.7b with resp 6: Shanghai\n",
      "qwen3:1.7b with resp 7: Shanghai\n",
      "qwen3:1.7b with resp 8: Shanghai\n",
      "qwen3:1.7b with resp 9: Shanghai\n",
      "qwen3:1.7b with resp 10: Shanghai\n",
      "total runtime of 10 responses of qwen3:1.7b is: 23.5 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import ollama\n",
    "\n",
    "# If you have proxy, make sure bypass proxies for local services (e.g., Ollama on localhost:11434)\n",
    "# You can check whether Ollama is running or not by tpying: http://127.0.0.1:11434/ in your Chrome.\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1\"\n",
    "os.environ[\"no_proxy\"] = \"localhost,127.0.0.1\"\n",
    "\n",
    "models = [\"qwen3:0.6b\", \"qwen3:1.7b\"]\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "\n",
    "for model in models:\n",
    "    print('-' * 50)\n",
    "    start_time = time.time()\n",
    "    for _ in range(10):\n",
    "        resp = ollama.generate(\n",
    "            model = model,\n",
    "            prompt = prompt\n",
    "        )\n",
    "        print(f\"{model} with resp {_ + 1}: {resp[\"response\"]}\")\n",
    "    print(f'total runtime of 10 responses of {model} is: {time.time() - start_time:.1f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b74f3-b395-4c63-8d2f-d71d9adb532f",
   "metadata": {},
   "source": [
    "**Some key observations:**\n",
    "\n",
    "- The smaller model Qwen3:0.6b produces lower quality answers while the relative bigger model Qwen3:1.7b produces high quality answers.\n",
    "- The response is a kind of random as each time the response may not be the same.\n",
    "\n",
    "Certainly, there are ways to make sure the above to generate a fixed answer. For example, you can use the following code where each time it always produce the maximal probability as the answer:\n",
    "\n",
    "```python\n",
    "resp = ollama.generate(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    options={\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 0,\n",
    "        # optional:\n",
    "        \"num_predict\": 32,\n",
    "    },\n",
    ")\n",
    "print(resp[\"response\"])\n",
    "```\n",
    "\n",
    "Let us generate some response that are not one word but a sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d3446ef-8081-45f8-8852-da387bb19b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I am an undergraduate student, please explain LLMs in three sentences. \n",
      "Resp: Large Language Models (LLMs) are AI systems trained on massive text datasets to understand and generate human-like text. They excel at tasks like writing essays, coding, and translating languages by learning patterns and context from vast amounts of data. LLMs rely on deep learning techniques, such as transformer architectures, to predict subsequent words or phrases, enabling them to perform complex tasks with remarkable accuracy.\n"
     ]
    }
   ],
   "source": [
    "model = \"qwen3:1.7b\"\n",
    "prompt = \"I am an undergraduate student, please explain LLMs in three sentences.\"\n",
    "resp = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt\n",
    "        )\n",
    "print(f\"Prompt: {prompt} \\nResp: {resp[\"response\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b2927-4bff-43b1-bb37-e11563bf44a4",
   "metadata": {},
   "source": [
    "### 1.3 Get response probability from LLMs\n",
    "\n",
    "From above outputs, you see that each time, the response of these LLMs could be different. Actually, all LLMs are probabilitic model where each time, they response (i.e., answers) prompts (i.e., questions) differently. We can use math language to describe this inference process.\n",
    "\n",
    "Let $p_\\theta(\\cdot)$ be the trained probability model. Here, you can think $p_\\theta(\\cdot)$ as Qwen3:0.6b, Qwen3:1.7b or any other models. Given the prompt *Fudan University is located in which city? Answer with one word.* (a sequence of tokens), the above response will give you an answer, which is also a sequence of tokens. So, the model will use an algorithm to predict the response given the prompt. Use the math language, we can think it tries to calculate the following probability:\n",
    "\n",
    "$$\n",
    "p_\\theta \\left(w_{n+1}|w_1,w_2,\\ldots, w_{n}\\right),\n",
    "$$\n",
    "where\n",
    "- Prompt=[*Fudan University is located in which city? Answer with one word.]* $=[w_1,w_2,\\ldots, w_{n}]$,\n",
    "- Response=[$w_{n+1}$].\n",
    "\n",
    "The model will try to output the most likeliy word $w_{n+1}$ using its own *inference algorithm*. We will detail this part in later lectures. By the definition of conditional probability, we may calculate it as\n",
    "\n",
    "$$\n",
    "p_\\theta \\left(w_{n+1}|w_1,w_2,\\ldots, w_{n}\\right) = \\frac{ p_\\theta \\left(w_1,w_2,\\ldots, w_{n}, w_{n+1}\\right)}{p_\\theta \\left(w_1,w_2,\\ldots, w_{n}\\right)}.\n",
    "$$\n",
    "\n",
    "The above is essential to say that, if we can learn a model that can represent any length sequence of distribution $p_\\theta \\left(w_1,w_2,\\ldots, w_{k}\\right)$, where $k$ could be any positive integer, then we can compute the conditional distribution easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "790f2408-7a79-4980-81c3-fa96f5059598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: 'Beijing'\n",
      "00 token=            'Be' logp=-0.7104 p=0.4914\n",
      "01 token=         'ijing' logp=-0.2248 p=0.7987\n"
     ]
    }
   ],
   "source": [
    "model = \"qwen3:0.6b\" # \"qwen3:1.7b\"\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "num_top_tokens = 20 # number of top alternatives per generated token\n",
    "resp = ollama.generate(\n",
    "    model = model,\n",
    "    prompt = prompt,\n",
    "    stream = False,\n",
    "    logprobs = True,\n",
    "    think = False,\n",
    "    top_logprobs = num_top_tokens\n",
    ")\n",
    "print(\"response:\", repr(resp[\"response\"]))\n",
    "\n",
    "# Each element usually corresponds to one generated token\n",
    "for i, lp in enumerate(resp.get(\"logprobs\", [])):\n",
    "    tok = lp.get(\"token\")\n",
    "    logp = lp.get(\"logprob\")\n",
    "    p = math.exp(logp) if logp is not None else None\n",
    "    print(f\"{i:02d} token={tok!r:>16} logp={logp: .4f} p={p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cbf85685-d462-4406-a7c5-c65de6f220d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use model: qwen3:0.6b\n",
      "--- top probabilities of token-00 ---\n",
      "        'Be':0.49277\n",
      "        'Sh':0.19577\n",
      "         'F':0.15628\n",
      "        'Ch':0.05104\n",
      "        'Gu':0.02999\n",
      "         'H':0.01773\n",
      "         'S':0.00861\n",
      "         'J':0.00580\n",
      "         'Y':0.00487\n",
      "         'B':0.00325\n",
      "Partial Response: Be\n",
      "\n",
      "--- top probabilities of token-01 ---\n",
      "     'ijing':0.79945\n",
      "        'ij':0.04640\n",
      "        'ih':0.03969\n",
      "     'iping':0.02474\n",
      "        'iz':0.01514\n",
      "        'id':0.01411\n",
      "        'il':0.01020\n",
      "        'iy':0.00864\n",
      "      'jing':0.00839\n",
      "        'ib':0.00624\n",
      "Partial Response: Beijing\n",
      "\n",
      "Final Response: Beijing\n"
     ]
    }
   ],
   "source": [
    "import os, math, ollama\n",
    "\n",
    "model = \"qwen3:0.6b\"\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "\n",
    "res = ollama.generate(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    logprobs=True,\n",
    "    think = False,\n",
    "    top_logprobs=10,\n",
    "    options={\"temperature\": 0.0, # greedy decoding, it pick the maximal token\n",
    "             \"num_predict\": 20,\n",
    "            \"think\": False # do not use thinking model.\n",
    "            },\n",
    ")\n",
    "\n",
    "answer = ''\n",
    "lp = res[\"logprobs\"]\n",
    "tokens = [d.get(\"token\", \"\") for d in lp]\n",
    "print(f'We use model: {model}')\n",
    "for i in range(len(lp)):\n",
    "    tok = lp[i].get(\"token\", \"\")\n",
    "    logp = lp[i].get(\"logprob\", None)\n",
    "    alts = lp[i].get(\"top_logprobs\", [])\n",
    "    p = math.exp(logp) if logp is not None else float(\"nan\")\n",
    "    if tok == \"\\n\" or tok == \"\\n\\n\": # stop when answer ends (often newline).\n",
    "        break\n",
    "    answer += tok\n",
    "    print(f\"--- top probabilities of token-{i:02d} ---\")\n",
    "    for a in alts[:20]:\n",
    "        prob_a = math.exp(a['logprob'])\n",
    "        print(f\"{a['token']!r:>12}:{prob_a:.5f}\")\n",
    "    print(f\"Partial Response: {answer}\\n\")\n",
    "print(f\"Final Response: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "02071500-6ccb-4bcb-89d7-d01f2e7322ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fudan University is located in **Shanghai**.\n"
     ]
    }
   ],
   "source": [
    "model = \"qwen3:1.7b\"\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "\n",
    "res = ollama.generate(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    logprobs=True,\n",
    "    think = False,\n",
    "    top_logprobs=10,\n",
    "    options={\"temperature\": 0.0, # greedy decoding, it pick the maximal token\n",
    "             \"num_predict\": 20,\n",
    "            \"think\": False # do not use thinking model.\n",
    "            },\n",
    ")\n",
    "print(res[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444aad1c-b734-480c-9db4-419682decc26",
   "metadata": {},
   "source": [
    "If we do not use thinking mode, it will gives the above response. However, we can still see how Shanghai is chosen during the inference stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c8fe4519-17fe-42f8-a2d7-dd419a3cdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use model: qwen3:1.7b\n",
      "--- top probabilities of token-00 ---\n",
      "         'F':0.99949\n",
      "        'An':0.00017\n",
      "        'Sh':0.00016\n",
      "        'Fu':0.00014\n",
      "         'W':0.00001\n",
      "         'Z':0.00001\n",
      "         'P':0.00000\n",
      "      'Hang':0.00000\n",
      "         'J':0.00000\n",
      "        'Be':0.00000\n",
      "Partial Response: F\n",
      "\n",
      "--- top probabilities of token-01 ---\n",
      "        'ud':0.99999\n",
      "        'uj':0.00001\n",
      "        'uz':0.00000\n",
      "     'uding':0.00000\n",
      "        'uy':0.00000\n",
      "       'udu':0.00000\n",
      "        'UD':0.00000\n",
      "       'uda':0.00000\n",
      "       'udd':0.00000\n",
      "      'udge':0.00000\n",
      "Partial Response: Fud\n",
      "\n",
      "--- top probabilities of token-02 ---\n",
      "        'an':1.00000\n",
      "       'ans':0.00000\n",
      "        'am':0.00000\n",
      "        '–∞–Ω':0.00000\n",
      "     'anian':0.00000\n",
      "       'anh':0.00000\n",
      "        'un':0.00000\n",
      "        'AN':0.00000\n",
      "        'on':0.00000\n",
      "       'ian':0.00000\n",
      "Partial Response: Fudan\n",
      "\n",
      "--- top probabilities of token-03 ---\n",
      "' University':1.00000\n",
      "'University':0.00000\n",
      "  ' Univers':0.00000\n",
      "     ' Univ':0.00000\n",
      " ' Universe':0.00000\n",
      "' Universities':0.00000\n",
      "' √úniversitesi':0.00000\n",
      "' Universit√§t':0.00000\n",
      "      ' Uni':0.00000\n",
      "       ' Un':0.00000\n",
      "Partial Response: Fudan University\n",
      "\n",
      "--- top probabilities of token-04 ---\n",
      "       ' is':1.00000\n",
      "       'ÊòØ‰∏≠ÂõΩ':0.00000\n",
      "         'ÊòØ':0.00000\n",
      "   ' adalah':0.00000\n",
      "       ' l√†':0.00000\n",
      "        ' ÊòØ':0.00000\n",
      "       '.is':0.00000\n",
      "        'is':0.00000\n",
      "       'ÊòØÊàëÂõΩ':0.00000\n",
      "      ' are':0.00000\n",
      "Partial Response: Fudan University is\n",
      "\n",
      "--- top probabilities of token-05 ---\n",
      "  ' located':1.00000\n",
      "  ' Located':0.00000\n",
      "        '‰Ωç‰∫é':0.00000\n",
      " ' situated':0.00000\n",
      "   'located':0.00000\n",
      "        '‰ΩçÊñº':0.00000\n",
      "   'Located':0.00000\n",
      "       ' in':0.00000\n",
      "   ' locate':0.00000\n",
      " ' location':0.00000\n",
      "Partial Response: Fudan University is located\n",
      "\n",
      "--- top probabilities of token-06 ---\n",
      "       ' in':1.00000\n",
      "         'Âú®':0.00000\n",
      "       ' ŸÅŸä':0.00000\n",
      "        ' –≤':0.00000\n",
      "       ' on':0.00000\n",
      "        '‡πÉ‡∏ô':0.00000\n",
      "        'in':0.00000\n",
      "       ' at':0.00000\n",
      "   ' within':0.00000\n",
      "       ' In':0.00000\n",
      "Partial Response: Fudan University is located in\n",
      "\n",
      "--- top probabilities of token-07 ---\n",
      "       ' **':1.00000\n",
      " ' Shanghai':0.00000\n",
      "       ' __':0.00000\n",
      "      ' the':0.00000\n",
      "   ' ______':0.00000\n",
      "        ' *':0.00000\n",
      "       ' Sh':0.00000\n",
      "      ' ***':0.00000\n",
      "     ' ****':0.00000\n",
      "     ' ____':0.00000\n",
      "Partial Response: Fudan University is located in **\n",
      "\n",
      "--- top probabilities of token-08 ---\n",
      "        'Sh':0.73826\n",
      "         'W':0.15449\n",
      "         'F':0.06032\n",
      "         'P':0.02518\n",
      "         'N':0.00847\n",
      "         'S':0.00207\n",
      "      'Ping':0.00169\n",
      "        'Gu':0.00096\n",
      "      'Hang':0.00095\n",
      " ' Shanghai':0.00095\n",
      "Partial Response: Fudan University is located in **Sh\n",
      "\n",
      "--- top probabilities of token-09 ---\n",
      "    'anghai':0.99938\n",
      "       'ang':0.00058\n",
      "    'enzhen':0.00001\n",
      "        'en':0.00001\n",
      "       'eng':0.00000\n",
      "        'an':0.00000\n",
      "       'eny':0.00000\n",
      "       'enz':0.00000\n",
      "       'ANG':0.00000\n",
      "       'aan':0.00000\n",
      "Partial Response: Fudan University is located in **Shanghai\n",
      "\n",
      "--- top probabilities of token-10 ---\n",
      "        '**':1.00000\n",
      "       '**,':0.00000\n",
      "       '***':0.00000\n",
      "       '**:':0.00000\n",
      "     ' City':0.00000\n",
      "       ' **':0.00000\n",
      "       ')**':0.00000\n",
      "      '****':0.00000\n",
      "     ' city':0.00000\n",
      "    '**\\n\\n':0.00000\n",
      "Partial Response: Fudan University is located in **Shanghai**\n",
      "\n",
      "--- top probabilities of token-11 ---\n",
      "         '.':0.99997\n",
      "     '.\\n\\n':0.00003\n",
      "       '.\\n':0.00000\n",
      "         'Ôºé':0.00000\n",
      "   '.\\n\\n\\n':0.00000\n",
      " '.\\n\\n\\n\\n':0.00000\n",
      "        '.<':0.00000\n",
      " '.\\r\\n\\r\\n':0.00000\n",
      "         '„ÄÇ':0.00000\n",
      "       '.</':0.00000\n",
      "Partial Response: Fudan University is located in **Shanghai**.\n",
      "\n",
      "Final Response: Fudan University is located in **Shanghai**.\n"
     ]
    }
   ],
   "source": [
    "import os, math, ollama\n",
    "\n",
    "model = \"qwen3:1.7b\"\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "\n",
    "res = ollama.generate(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    logprobs=True,\n",
    "    think = False, # Do not use the thinking model.\n",
    "    top_logprobs=10,\n",
    "    options={\"temperature\": 0.0, # greedy decoding, it pick the maximal token\n",
    "             \"num_predict\": 20,\n",
    "            \"think\": False # do not use thinking model.\n",
    "            },\n",
    ")\n",
    "\n",
    "answer = ''\n",
    "lp = res[\"logprobs\"]\n",
    "tokens = [d.get(\"token\", \"\") for d in lp]\n",
    "print(f'We use model: {model}')\n",
    "for i in range(len(lp)):\n",
    "    tok = lp[i].get(\"token\", \"\")\n",
    "    logp = lp[i].get(\"logprob\", None)\n",
    "    alts = lp[i].get(\"top_logprobs\", [])\n",
    "    p = math.exp(logp) if logp is not None else float(\"nan\")\n",
    "    if tok == \"\\n\" or tok == \"\\n\\n\": # stop when answer ends (often newline).\n",
    "        break\n",
    "    answer += tok\n",
    "    print(f\"--- top probabilities of token-{i:02d} ---\")\n",
    "    for a in alts[:20]:\n",
    "        prob_a = math.exp(a['logprob'])\n",
    "        print(f\"{a['token']!r:>12}:{prob_a:.5f}\")\n",
    "    print(f\"Partial Response: {answer}\\n\")\n",
    "print(f\"Final Response: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21141a2-a2d4-457c-a519-c231c0fcf0cf",
   "metadata": {},
   "source": [
    "## 2. Histroy of NLP and LLMs\n",
    "\n",
    "Please check this part in our slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08ee8c-34b5-4373-a2cc-9ed8b929066a",
   "metadata": {},
   "source": [
    "## 3. Basics for Python and spaCy\n",
    "\n",
    "- **Python**: We will use Python-3.12 in our course.\n",
    "- **spaCy**: As introduced in [https://github.com/explosion/spaCy](https://github.com/explosion/spaCy), [spaCy](https://spacy.io/) is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. We will use it to demonostrate how to do text tokenization.\n",
    "- **nltk tool**: In our previous courses, we will introduce nltk tool for tokenization stuff. But we will not cover this part in our new course as these tools are largely iirelvant and outdated. One can find details of nltk at [https://github.com/nltk/nltk](https://github.com/nltk/nltk) and website at [https://www.nltk.org/](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301e767-854c-43de-a7c6-6e1e3528ddcd",
   "metadata": {},
   "source": [
    "### 3.1 Python basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c51f651c-86f1-4274-ac7f-2ef1926d019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python, there is a built in lib re, we can import them\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c4a56fac-f9f2-470e-a706-4fea0a6091fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(21, 30), match='Woodchuck'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Task: Find woodchuck or Woodchuck : Disjunction\n",
    "test_str = \"This string contains Woodchuck and woodchuck.\"\n",
    "result=re.search(pattern=\"[wW]oodchuck\", string=test_str)\n",
    "print(result)\n",
    "result=re.search(pattern=r\"[wW]ooodchuck\", string=test_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "37e1e739-90fc-41a7-8781-f35b5ce223af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(21, 30), match='woodchuck'>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the word \"woodchuck\" in the following test string\n",
    "test_str = \"interesting links to woodchucks ! and lemurs!\"\n",
    "re.search(pattern=\"woodchuck\", string=test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d3bb804d-bdc0-471c-8606-bb178bfa3f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(32, 33), match='!'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Find !, it follows the same way:\n",
    "print(re.search(pattern=\"!\", string=test_str))\n",
    "print(re.search(pattern=\"!!\", string=test_str))\n",
    "assert re.search(pattern=\"!!\", string=test_str) == None # match nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "17845520-c90c-4a20-bbd5-8304b983f3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(10, 11), match='7'>\n",
      "<re.Match object; span=(10, 11), match='7'>\n"
     ]
    }
   ],
   "source": [
    "# Find any single digit in a string.\n",
    "result=re.search(pattern=r\"[0123456789]\", string=\"plenty of 7 to 5\")\n",
    "print(result)\n",
    "result=re.search(pattern=r\"[0-9]\", string=\"plenty of 7 to 5\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6896eb05-f93d-475c-a9de-30f20117457c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 2), match='y'>\n",
      "<re.Match object; span=(0, 1), match='I'>\n",
      "<re.Match object; span=(0, 1), match='o'>\n",
      "<re.Match object; span=(8, 9), match='^'>\n",
      "<re.Match object; span=(8, 11), match='a^b'>\n"
     ]
    }
   ],
   "source": [
    "# Negation: If the caret ^ is the first symbol after [,\n",
    "# the resulting pattern is negated. For example, the pattern \n",
    "# [^a] matches any single character (including special characters) except a.\n",
    "\n",
    "# -- not an upper case letter\n",
    "print(re.search(pattern=r\"[^A-Z]\", string=\"Oyfn pripetchik\"))\n",
    "\n",
    "# -- neither 'S' nor 's'\n",
    "print(re.search(pattern=r\"[^Ss]\", string=\"I have no exquisite reason for't\"))\n",
    "\n",
    "# -- not a period\n",
    "print(re.search(pattern=r\"[^.]\", string=\"our resident Djinn\"))\n",
    "\n",
    "# -- either 'e' or '^'\n",
    "print(re.search(pattern=r\"[e^]\", string=\"look up ^ now\"))\n",
    "\n",
    "# -- the pattern ‚Äòa^b‚Äô\n",
    "print(re.search(pattern=r'a\\^b', string=r'look up a^b now'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9f86f6ca-4d0f-4bfd-9d58-05a2cb71fa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(31, 40), match='groundhog'>\n"
     ]
    }
   ],
   "source": [
    "# More disjuncations\n",
    "str1 = \"Woodchucks is another name for groundhog!\"\n",
    "result = re.search(pattern=\"groundhog|woodchuck\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "12e33f63-ff31-4c0e-8a3d-fd692acd7715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woodchuck', 'Woodchuck', 'Groundhog', 'groundhog']\n"
     ]
    }
   ],
   "source": [
    "str1 = \"Find all woodchuckk Woodchuck Groundhog groundhogxxx!\"\n",
    "result = re.findall(pattern=\"[gG]roundhog|[Ww]oodchuck\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d6fba48-2c61-4cfe-9f96-8c9aa1f4475d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color', 'colour']\n",
      "['color', 'colour', 'colouur', 'colouuur']\n",
      "['baa', 'baaa', 'baaaa', 'baaaaa']\n",
      "['begin', 'begun', 'begun', 'beg3n']\n",
      "['.']\n",
      "['t']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:22: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/5f/z4gt_kmj3qj9rhs1z8yl7nxr0000gn/T/ipykernel_8443/824913852.py:22: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  result = re.findall(pattern=\"\\.$\",string=str1)\n"
     ]
    }
   ],
   "source": [
    "# Some special chars\n",
    "\n",
    "# ?: Optional previous char\n",
    "str1 = \"Find all color colour colouur colouuur colouyr\"\n",
    "result = re.findall(pattern=\"colou?r\",string=str1)\n",
    "print(result)\n",
    "\n",
    "# *: 0 or more of previous char\n",
    "str1 = \"Find all color colour colouur colouuur colouyr\"\n",
    "result = re.findall(pattern=\"colou*r\",string=str1)\n",
    "print(result)\n",
    "\n",
    "# +: 1 or more of previous char\n",
    "str1 = \"baa baaa baaaa baaaaa\"\n",
    "result = re.findall(pattern=\"baa+\",string=str1)\n",
    "print(result)\n",
    "# .: any char\n",
    "str1 = \"begin begun begun beg3n\"\n",
    "result = re.findall(pattern=\"beg.n\",string=str1)\n",
    "print(result)\n",
    "str1 = \"The end.\"\n",
    "result = re.findall(pattern=\"\\.$\",string=str1)\n",
    "print(result)\n",
    "str1 = \"The end? The end. #t\"\n",
    "result = re.findall(pattern=\".$\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b00dcb9e-09bb-4e8a-af1c-f3b53eca5391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' the ', ' the ', ' the ', ' The ', ' the ', ' the ', ' the ', ' the ']\n"
     ]
    }
   ],
   "source": [
    "# find all \"the\" in a raw text.\n",
    "text = \"If two sequences in an alignment share a common ancestor, \\\n",
    "mismatches can be interpreted as point mutations and gaps as indels (that \\\n",
    "is, insertion or deletion mutations) introduced in one or both lineages in \\\n",
    "the time since they diverged from one another. In sequence alignments of \\\n",
    "proteins, the degree of similarity between amino acids occupying a \\\n",
    "particular position in the sequence can be interpreted as a rough \\\n",
    "measure of how conserved a particular region or sequence motif is \\\n",
    "among lineages. The absence of substitutions, or the presence of \\\n",
    "only very conservative substitutions (that is, the substitution of \\\n",
    "amino acids whose side chains have similar biochemical properties) in \\\n",
    "a particular region of the sequence, suggest [3] that this region has \\\n",
    "structural or functional importance. Although DNA and RNA nucleotide bases \\\n",
    "are more similar to each other than are amino acids, the conservation of \\\n",
    "base pairs can indicate a similar functional or structural role.\"\n",
    "matches = re.findall(\"[^a-zA-Z][tT]he[^a-zA-Z]\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1d9444c5-bf35-4947-8128-428fb9a9cae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'the', 'the', 'The', 'the', 'the', 'the', 'the']\n"
     ]
    }
   ],
   "source": [
    "# A nicer way is to do the following\n",
    "\n",
    "matches = re.findall(r\"\\b[tT]he\\b\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "007b2336-eac6-4b58-bdbe-65609fcdca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikilink_re():\n",
    "    \"\"\" This regex is from the following Github:\n",
    "    https://github.com/WikiLinkGraphs/wikidump\n",
    "    \"\"\"\n",
    "    regex_str = r'''(?P<total>(?P<wikilink>\n",
    "        \\[\\[(?P<link>[ÀÜ\\n\\|\\]\\[\\<\\>\\{\\}]{0,256})(?:\\|(?P<anchor>[ÀÜ\\[]*?))?\\]\\])\\w*)\\s?'''\n",
    "    return regex.compile(regex_str, regex.VERBOSE | regex.MULTILINE)\n",
    "\n",
    "# Task: Implement the task shown in Slides 52\n",
    "# You may need to\n",
    "# 1. Download a Wikipedia article xml file\n",
    "# 2. Use RE to extract links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b7609-85dd-47f7-b878-1f99f97c5f07",
   "metadata": {},
   "source": [
    "### 3.2 spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6b80de04-10da-4216-be12-5f10041cfa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
      "\n",
      "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
      "\n",
      "  conda config --add channels defaults\n",
      "\n",
      "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
      "\n",
      "  deprecated.topic(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
      "\n",
      "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
      "\n",
      "  conda config --add channels defaults\n",
      "\n",
      "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
      "\n",
      "  deprecated.topic(\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3/envs/llm-26\n",
      "\n",
      "  added / updated specs:\n",
      "    - spacy\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/osx-arm64::certifi-2025.11.~ --> conda-forge/noarch::certifi-2025.11.12-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "## install spaCy\n",
    "!conda install -c conda-forge spacy -y --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6fd509-59b4-4678-8454-53d12d24d9ad",
   "metadata": {},
   "source": [
    "Open your terminal and activate your llm-26 env, and then run the following to download English LMs.\n",
    "```python\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "94ed8b2f-c161-4574-b1d2-863d97f01b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- token: Fudan ---\n",
      "lemma: Fudan\n",
      "pos: PROPN\n",
      "tag: NNP\n",
      "dep: compound\n",
      "shape: Xxxxx\n",
      "is_alpha:True\n",
      "is_stop: False\n",
      "--- token: University ---\n",
      "lemma: University\n",
      "pos: PROPN\n",
      "tag: NNP\n",
      "dep: nsubjpass\n",
      "shape: Xxxxx\n",
      "is_alpha:True\n",
      "is_stop: False\n",
      "--- token: is ---\n",
      "lemma: be\n",
      "pos: AUX\n",
      "tag: VBZ\n",
      "dep: auxpass\n",
      "shape: xx\n",
      "is_alpha:True\n",
      "is_stop: True\n",
      "--- token: located ---\n",
      "lemma: locate\n",
      "pos: VERB\n",
      "tag: VBN\n",
      "dep: ROOT\n",
      "shape: xxxx\n",
      "is_alpha:True\n",
      "is_stop: False\n",
      "--- token: in ---\n",
      "lemma: in\n",
      "pos: ADP\n",
      "tag: IN\n",
      "dep: prep\n",
      "shape: xx\n",
      "is_alpha:True\n",
      "is_stop: True\n",
      "--- token: which ---\n",
      "lemma: which\n",
      "pos: DET\n",
      "tag: WDT\n",
      "dep: det\n",
      "shape: xxxx\n",
      "is_alpha:True\n",
      "is_stop: True\n",
      "--- token: city ---\n",
      "lemma: city\n",
      "pos: NOUN\n",
      "tag: NN\n",
      "dep: pobj\n",
      "shape: xxxx\n",
      "is_alpha:True\n",
      "is_stop: False\n",
      "--- token: ? ---\n",
      "lemma: ?\n",
      "pos: PUNCT\n",
      "tag: .\n",
      "dep: punct\n",
      "shape: ?\n",
      "is_alpha:False\n",
      "is_stop: False\n",
      "--- token: Answer ---\n",
      "lemma: answer\n",
      "pos: VERB\n",
      "tag: VB\n",
      "dep: ROOT\n",
      "shape: Xxxxx\n",
      "is_alpha:True\n",
      "is_stop: False\n",
      "--- token: with ---\n",
      "lemma: with\n",
      "pos: ADP\n",
      "tag: IN\n",
      "dep: prep\n",
      "shape: xxxx\n",
      "is_alpha:True\n",
      "is_stop: True\n",
      "--- token: one ---\n",
      "lemma: one\n",
      "pos: NUM\n",
      "tag: CD\n",
      "dep: nummod\n",
      "shape: xxx\n",
      "is_alpha:True\n",
      "is_stop: True\n",
      "--- token: word ---\n",
      "lemma: word\n",
      "pos: NOUN\n",
      "tag: NN\n",
      "dep: pobj\n",
      "shape: xxxx\n",
      "is_alpha:True\n",
      "is_stop: False\n",
      "--- token: . ---\n",
      "lemma: .\n",
      "pos: PUNCT\n",
      "tag: .\n",
      "dep: punct\n",
      "shape: .\n",
      "is_alpha:False\n",
      "is_stop: False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(prompt) # i want to do text preprocessing for our prompt.\n",
    "\n",
    "# Text: The original word text.\n",
    "# Lemma: The base form of the word.\n",
    "# POS: The simple UPOS part-of-speech tag.\n",
    "# Tag: The detailed part-of-speech tag.\n",
    "# Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "# Shape: The word shape ‚Äì capitalization, punctuation, digits.\n",
    "# is alpha: Is the token an alpha character? (whether it consists only of letters from the alphabet (A-Z or a-z))\n",
    "# is stop: Is the token part of a stop list, i.e. the most common words of the language? \n",
    "#         (A stop list (or stopwords list) is a list of commonly used words in a language that \n",
    "#         are usually ignored during natural language processing (NLP) tasks, such as text analysis or machine learning.)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"--- token: {token.text} ---\")\n",
    "    print(f\"lemma: {token.lemma_}\\npos: {token.pos_}\\ntag: {token.tag_}\\ndep: {token.dep_}\\nshape: {token.shape_}\\nis_alpha:{token.is_alpha}\\nis_stop: {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c357e-284f-4c1f-9e7d-08c64584249f",
   "metadata": {},
   "source": [
    "There are two type of tokenizations\n",
    "\n",
    "- **Top-down tokenization**: We define a standard and implement rules to implement that kind of tokenization.\n",
    "  - word tokenization\n",
    "  - charater tokenization\n",
    "- **Bottom-up tokenization**: We use simple statistics of letter sequences to break up words into subword tokens.\n",
    "  - subword tokenization (modern LLMs use this type!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "436fb389-51e6-4bc6-afab-473dd00798ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['While', 'the', 'Unix', 'command', 'sequence', 'just', 'removed', 'all', 'the', 'numbers', 'and', 'punctuation']\n"
     ]
    }
   ],
   "source": [
    "# Use split method via the whitespace \" \"\n",
    "text = \"\"\"While the Unix command sequence just removed all the numbers and punctuation\"\"\"\n",
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "988b4e05-d517-46d4-9c98-59a40a1d242c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'you', 'love', 'ü§ó', 'Transformers?', 'We', 'sure', 'do.']\n"
     ]
    }
   ],
   "source": [
    "# But, we have punctuations, icons, and many other small issues.\n",
    "text = \"\"\"Don't you love ü§ó Transformers? We sure do.\"\"\"\n",
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e9697946-d678-455d-920e-d73e1cb10daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Do, n't, you, love, ü§ó, Transformers, ?, We, sure, do, .]\n"
     ]
    }
   ],
   "source": [
    "# spacy works much better\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2fb21874-ac5c-464d-96da-267865244594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Special, characters, and, numbers, will, need, to, be, kept, in, prices, (, $, 45.55, ), and, dates, (, 01/02/06, ), ;, \n",
      ", we, do, n‚Äôt, want, to, segment, that, price, into, separate, tokens, of, ‚Äú, 45, ‚Äù, and, ‚Äú, 55, ‚Äù, ., And, there, are, URLs, (, https://www.stanford.edu, ), ,, \n",
      ", Twitter, hashtags, (, #, nlproc, ), ,, or, email, addresses, (, someone@cs.colorado.edu, ), .]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Special characters and numbers will need to be kept in prices ($45.55) and dates (01/02/06); \n",
    "we don‚Äôt want to segment that price into separate tokens of ‚Äú45‚Äù and ‚Äú55‚Äù. And there are URLs (https://www.stanford.edu),\n",
    "Twitter hashtags (#nlproc), or email addresses (someone@cs.colorado.edu).\"\"\"\n",
    "doc = nlp(text)\n",
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ddcb2-2b96-479d-9424-641cd05dfe49",
   "metadata": {},
   "source": [
    "Please install zh tokenization via spaCy: \n",
    "```python\n",
    "python -m spacy download zh_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "37ab54b4-006b-4422-ac60-d796944abb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ÂßöÊòé, ËøõÂÖ•, ÊÄªÂÜ≥Ëµõ]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "text = 'ÂßöÊòéËøõÂÖ•ÊÄªÂÜ≥Ëµõ'\n",
    "doc = nlp(text)\n",
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ad1b583e-4672-4574-bfbe-cff0ed083cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1Êúà', '1Êó•', 'Ôºå', 'ÂõΩÂä°Èô¢', 'ÂõΩËµÑÂßî', 'ÂÖ¨Â∏É', '‚Äú', '2025', 'Âπ¥Â∫¶', 'Â§Æ‰ºÅ', 'ÂçÅÂ§ß', 'ÂõΩ‰πãÈáçÂô®', '‚Äù', 'Ôºå', '1Êúà', '2Êó•', 'ÂÖ¨Â∏É', '‚Äú', '2025', 'Âπ¥Â∫¶', 'Â§Æ‰ºÅ', 'ÂçÅÂ§ß', 'Ë∂ÖÁ∫ß', 'Â∑•Á®ã', '‚Äù', '„ÄÇ', '\\u200c\\u200c']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"1Êúà1Êó•ÔºåÂõΩÂä°Èô¢ÂõΩËµÑÂßîÂÖ¨Â∏É‚Äú2025Âπ¥Â∫¶Â§Æ‰ºÅÂçÅÂ§ßÂõΩ‰πãÈáçÂô®‚ÄùÔºå1Êúà2Êó•ÂÖ¨Â∏É‚Äú2025Âπ¥Â∫¶Â§Æ‰ºÅÂçÅÂ§ßË∂ÖÁ∫ßÂ∑•Á®ã‚Äù„ÄÇ‚Äå‚Äå\"\"\"\n",
    "doc = nlp(text)\n",
    "print([str(token) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fbbcb565-de3a-42b8-82bf-f1fdb294129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, Êúà, 1, Êó•, Ôºå, ÂõΩ, Âä°, Èô¢, ÂõΩ, ËµÑ, Âßî, ÂÖ¨, Â∏É, ‚Äú, 2, 0, 2, 5, Âπ¥, Â∫¶, Â§Æ, ‰ºÅ, ÂçÅ, Â§ß, ÂõΩ, ‰πã, Èáç, Âô®, ‚Äù, Ôºå, 1, Êúà, 2, Êó•, ÂÖ¨, Â∏É, ‚Äú, 2, 0, 2, 5, Âπ¥, Â∫¶, Â§Æ, ‰ºÅ, ÂçÅ, Â§ß, Ë∂Ö, Á∫ß, Â∑•, Á®ã, ‚Äù, „ÄÇ, ‚Äå, ‚Äå]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.zh import Chinese\n",
    "nlp_ch = Chinese()\n",
    "print([*nlp_ch(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24912cb5-b719-458a-80e9-c43dedfdd887",
   "metadata": {},
   "source": [
    "**Lemmatization (ËØçÂΩ¢ËøòÂéü)**\n",
    "\n",
    "- Lemmatization is the task of determining that two words have the same root, despite their surface differences.\n",
    "- **Motivation**: For some NLP situations, we also want two morphologically different forms of a word to behave similarly. For example in web search, someone may type the string woodchucks but a useful system might want to also return pages\n",
    "that mention woodchuck with no s.\n",
    "- **Example 1**: The words am, are, and is have the shared lemma be.\n",
    "- **Example 2**: The words dinner and dinners both have the lemma dinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1107c736-b9ab-4116-8f21-803da85ef2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Brown Corpus, a text corpus of American English that was compiled in the 1960s at Brown University, is widely used in the field of linguistics and natural language processing. It contains about 1 million words (or \"tokens\") across a diverse range of texts from 500 sources, categorized into 15 genres, such as news, editorial, and fiction, to provide a comprehensive resource for studying the English language. This corpus has been instrumental in the development and evaluation of various computational linguistics algorithms and tools.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "The Brown Corpus, a text corpus of American English that was compiled in the 1960s at Brown University, \\\n",
    "is widely used in the field of linguistics and natural language processing. It contains about 1 million \\\n",
    "words (or \"tokens\") across a diverse range of texts from 500 sources, categorized into 15 genres, such \\\n",
    "as news, editorial, and fiction, to provide a comprehensive resource for studying the English language. \\\n",
    "This corpus has been instrumental in the development and evaluation of various computational linguistics \\\n",
    "algorithms and tools.\n",
    "\"\"\"\n",
    "text = text.replace(\"\\n\", \" \").strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b44864d0-0da0-4ead-80f3-34b47e570a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "print(doc[0], type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "46080ca6-20b0-4350-942c-64421cddcc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the\n",
      "Brown Brown\n",
      "Corpus Corpus\n",
      ", ,\n",
      "a a\n",
      "text text\n",
      "corpus corpus\n",
      "of of\n",
      "American American\n",
      "English English\n",
      "that that\n",
      "was be\n",
      "compiled compile\n",
      "in in\n",
      "the the\n",
      "1960s 1960\n",
      "at at\n",
      "Brown Brown\n",
      "University University\n",
      ", ,\n",
      "is be\n",
      "widely widely\n",
      "used use\n",
      "in in\n",
      "the the\n",
      "field field\n",
      "of of\n",
      "linguistics linguistic\n",
      "and and\n",
      "natural natural\n"
     ]
    }
   ],
   "source": [
    "lemmas = [token.lemma_ for token in doc]\n",
    "for ori,lemma in zip(doc[:30], lemmas[:30]):\n",
    "    print(ori, lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8d4fd-fb05-410f-a140-ffda88850934",
   "metadata": {},
   "source": [
    "**Stemming (ËØçÂπ≤ÊèêÂèñ)**\n",
    "\n",
    "The Porter-Stemmer method\n",
    "\n",
    "Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of chopping off words final affixes. This naive version of morphological analysis is called stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4651f6e4-65b0-4d6f-8b92-d9b19cad112b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This this\n",
      "was be\n",
      "not not\n",
      "the the\n",
      "map map\n",
      "we we\n",
      "found find\n",
      "in in\n",
      "Billy Billy\n",
      "Bones Bones\n",
      "chest chest\n",
      "but but\n",
      "an an\n",
      "accurate accurate\n",
      "copy copy\n",
      "complete complete\n",
      "in in\n",
      "all all\n",
      "things thing\n",
      "names name\n",
      "and and\n",
      "heights height\n",
      "and and\n",
      "soundings sounding\n",
      "with with\n",
      "the the\n",
      "single single\n",
      "exception exception\n",
      "of of\n",
      "the the\n",
      "red red\n",
      "crosses cross\n",
      "and and\n",
      "the the\n",
      "written write\n",
      "notes note\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"\\\n",
    "This was not the map we found in Billy Bones's chest, but \\\n",
    "an accurate copy, complete in all things-names and heights \\\n",
    "and soundings-with the single exception of the red crosses \\\n",
    "and the written notes.\\\n",
    "\"\"\"   \n",
    "doc = nlp(text)\n",
    "\n",
    "for tok in doc:\n",
    "    if tok.is_alpha:\n",
    "        print(tok.text, tok.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a0a3d84a-e5cf-453e-86a6-cfee5287bebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-0: Here is a sentence.\n",
      "\n",
      "sentence-1: Here is another one!\n",
      "\n",
      "sentence-2: And the last one.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A modern and fast NLP library that includes support for sentence segmentation. \n",
    "# spaCy uses a statistical model to predict sentence boundaries, which can be more accurate \n",
    "# than rule-based approaches for complex texts.\n",
    "# Install via conda: conda install conda-forge::spacy\n",
    "# Install via pip:   pip install -U spacy\n",
    "# Download data: python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Here is a sentence. Here is another one! And the last one.\")\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5a694a9e-f604-4824-a17d-09d7281c4c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-0: Â≠óËäÇÂØπÁºñÁ†ÅÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÊï∞ÊçÆÂéãÁº©ÂΩ¢ÂºèÔºåËøôÁßçÊñπÊ≥ïÁî®Êï∞ÊçÆ‰∏≠‰∏çÂ≠òÁöÑ‰∏Ä‰∏™Â≠óËäÇË°®Á§∫ÊúÄÂ∏∏Âá∫Áé∞ÁöÑËøûÁª≠Â≠óËäÇÊï∞ÊçÆ„ÄÇ\n",
      "\n",
      "sentence-1: ËøôÊ†∑ÁöÑÊõøÊç¢ÈúÄË¶ÅÈáçÂª∫ÂÖ®ÈÉ®ÂéüÂßãÊï∞ÊçÆ„ÄÇ\n",
      "\n",
      "sentence-2: Â≠óËäÇÂØπÁºñÁ†ÅÂÆû‰æã: ÂÅáËÆæÊàë‰ª¨Ë¶ÅÁºñÁ†ÅÊï∞ÊçÆ aaabdaaabac, Â≠óËäÇÂØπ‚Äúaa‚ÄùÂá∫Áé∞Ê¨°Êï∞ÊúÄÂ§öÔºåÊâÄ‰ª•Êàë‰ª¨Áî®Êï∞ÊçÆ‰∏≠Ê≤°ÊúâÂá∫Áé∞ÁöÑÂ≠óËäÇ‚ÄúZ‚ÄùÊõøÊç¢‚Äúaa‚ÄùÂæóÂà∞ÊõøÊç¢Ë°®\n",
      "Z <- aa Êï∞ÊçÆËΩ¨Âèò‰∏∫ ZabdZabac.\n",
      "\n",
      "sentence-3: Âú®Ëøô‰∏™Êï∞ÊçÆ‰∏≠ÔºåÂ≠óËäÇÂØπ‚ÄúZa‚ÄùÂá∫Áé∞ÁöÑÊ¨°Êï∞ÊúÄÂ§öÔºåÊàë‰ª¨Áî®Âè¶Â§ñ‰∏Ä‰∏™Â≠óËäÇ‚ÄúY‚ÄùÊù•ÊõøÊç¢ÂÆÉÔºàËøôÁßçÊÉÖÂÜµ‰∏ãÁî±‰∫éÊâÄÊúâÁöÑ‚ÄúZ‚ÄùÈÉΩÂ∞ÜË¢´ÊõøÊç¢ÔºåÊâÄ‰ª•‰πüÂèØ‰ª•Áî®‚ÄúZ‚ÄùÊù•ÊõøÊç¢‚ÄúZa‚ÄùÔºâÔºåÂæóÂà∞ÊõøÊç¢Ë°®‰ª•ÂèäÊï∞ÊçÆ\n",
      "Z <- aa, Y <- Za, YbdYbac.\n",
      "\n",
      "sentence-4: Êàë‰ª¨ÂÜçÊ¨°ÊõøÊç¢ÊúÄÂ∏∏Âá∫Áé∞ÁöÑÂ≠óËäÇÂØπÂæóÂà∞ÔºöZ <- aa, Y <- Za, X <-\n",
      "\n",
      "sentence-5: Yb.\n",
      "\n",
      "sentence-6: XdXac Áî±‰∫é‰∏çÂÜçÊúâÈáçÂ§çÂá∫Áé∞ÁöÑÂ≠óËäÇÂØπÔºåÊâÄ‰ª•Ëøô‰∏™Êï∞ÊçÆ‰∏çËÉΩÂÜçË¢´Ëøõ‰∏ÄÊ≠•ÂéãÁº©„ÄÇ\n",
      "\n",
      "sentence-7: Ëß£ÂéãÁöÑÊó∂ÂÄôÔºåÂ∞±ÊòØÊåâÁÖßÁõ∏ÂèçÁöÑÈ°∫Â∫èÊâßË°åÊõøÊç¢ËøáÁ®ã„ÄÇ\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # You need to install it via: python -m spacy download zh_core_web_sm\n",
    "from spacy.lang.zh.examples import sentences \n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "text = \"\"\"\\\n",
    "Â≠óËäÇÂØπÁºñÁ†ÅÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÊï∞ÊçÆÂéãÁº©ÂΩ¢ÂºèÔºåËøôÁßçÊñπÊ≥ïÁî®Êï∞ÊçÆ‰∏≠‰∏çÂ≠òÁöÑ‰∏Ä‰∏™Â≠óËäÇË°®Á§∫ÊúÄÂ∏∏Âá∫Áé∞ÁöÑËøûÁª≠Â≠óËäÇÊï∞ÊçÆ„ÄÇËøôÊ†∑ÁöÑÊõøÊç¢ÈúÄË¶ÅÈáçÂª∫ÂÖ®ÈÉ®ÂéüÂßãÊï∞ÊçÆ„ÄÇÂ≠óËäÇÂØπÁºñÁ†ÅÂÆû‰æã: ÂÅáËÆæÊàë‰ª¨Ë¶ÅÁºñÁ†ÅÊï∞ÊçÆ aaabdaaabac, Â≠óËäÇÂØπ‚Äúaa‚ÄùÂá∫Áé∞Ê¨°Êï∞ÊúÄÂ§öÔºåÊâÄ‰ª•Êàë‰ª¨Áî®Êï∞ÊçÆ‰∏≠Ê≤°ÊúâÂá∫Áé∞ÁöÑÂ≠óËäÇ‚ÄúZ‚ÄùÊõøÊç¢‚Äúaa‚ÄùÂæóÂà∞ÊõøÊç¢Ë°®\n",
    "Z <- aa Êï∞ÊçÆËΩ¨Âèò‰∏∫ ZabdZabac. Âú®Ëøô‰∏™Êï∞ÊçÆ‰∏≠ÔºåÂ≠óËäÇÂØπ‚ÄúZa‚ÄùÂá∫Áé∞ÁöÑÊ¨°Êï∞ÊúÄÂ§öÔºåÊàë‰ª¨Áî®Âè¶Â§ñ‰∏Ä‰∏™Â≠óËäÇ‚ÄúY‚ÄùÊù•ÊõøÊç¢ÂÆÉÔºàËøôÁßçÊÉÖÂÜµ‰∏ãÁî±‰∫éÊâÄÊúâÁöÑ‚ÄúZ‚ÄùÈÉΩÂ∞ÜË¢´ÊõøÊç¢ÔºåÊâÄ‰ª•‰πüÂèØ‰ª•Áî®‚ÄúZ‚ÄùÊù•ÊõøÊç¢‚ÄúZa‚ÄùÔºâÔºåÂæóÂà∞ÊõøÊç¢Ë°®‰ª•ÂèäÊï∞ÊçÆ\n",
    "Z <- aa, Y <- Za, YbdYbac. Êàë‰ª¨ÂÜçÊ¨°ÊõøÊç¢ÊúÄÂ∏∏Âá∫Áé∞ÁöÑÂ≠óËäÇÂØπÂæóÂà∞ÔºöZ <- aa, Y <- Za, X <- Yb. XdXac Áî±‰∫é‰∏çÂÜçÊúâÈáçÂ§çÂá∫Áé∞ÁöÑÂ≠óËäÇÂØπÔºåÊâÄ‰ª•Ëøô‰∏™Êï∞ÊçÆ‰∏çËÉΩÂÜçË¢´Ëøõ‰∏ÄÊ≠•ÂéãÁº©„ÄÇËß£ÂéãÁöÑÊó∂ÂÄôÔºåÂ∞±ÊòØÊåâÁÖßÁõ∏ÂèçÁöÑÈ°∫Â∫èÊâßË°åÊõøÊç¢ËøáÁ®ã„ÄÇ\n",
    "\"\"\"\n",
    "doc = nlp(text)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6033d4-5b9c-4d1d-bd66-8a97a22c0fa5",
   "metadata": {},
   "source": [
    "## 4. LLMs tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "11028936-e87f-4208-a65a-6f91eb2870ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
      "\n",
      "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
      "\n",
      "  conda config --add channels defaults\n",
      "\n",
      "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
      "\n",
      "  deprecated.topic(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
      "\n",
      "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
      "\n",
      "  conda config --add channels defaults\n",
      "\n",
      "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
      "\n",
      "  deprecated.topic(\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3/envs/llm-26\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda-forge::tiktoken\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    regex-2025.11.3            |  py312haa24f5a_0         363 KB\n",
      "    tiktoken-0.12.0            |  py312hab2ecbf_3         799 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  regex              pkgs/main/osx-arm64::regex-2025.11.3-py312haa24f5a_0 \n",
      "  tiktoken           conda-forge/osx-arm64::tiktoken-0.12.0-py312hab2ecbf_3 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2025.11.1~ --> pkgs/main/osx-arm64::certifi-2025.11.12-py312hca03da5_0 \n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install conda-forge::tiktoken -y --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1465086a-ca17-4aac-b649-237d9dba5cff",
   "metadata": {},
   "source": [
    "### 4.1 Subword tokenization: BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe86061-f066-45e3-a2ff-b672a079808a",
   "metadata": {},
   "source": [
    "### 4.4 Huggingface tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6b77b-fd37-4bfd-ba5c-537896f0a51d",
   "metadata": {},
   "source": [
    "PreTrainedTokenizer, PreTrainedTokenizerBase, AutoTokenizer\n",
    "\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils.py\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_fast.py\n",
    "- Check tokenizers at https://github.com/huggingface/transformers/blob/main/setup.py\n",
    "- If you want to train a tokenizer by yourself, then go to: https://github.com/huggingface/tokenizers\n",
    "- A fast BPE tokenizer is also at: https://github.com/openai/tiktoken\n",
    "- An implementation of sentencepiece is at: https://github.com/google/sentencepiece\n",
    "- There are 3 most common methods for tokenization: https://github.com/huggingface/tokenizers/tree/main/tokenizers/src/models\n",
    "- - BPE: https://aclanthology.org/P16-1162.pdf\n",
    "  - Unigram: https://arxiv.org/pdf/1804.10959\n",
    "  - WordPiece https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec32b0-3e36-4668-94a9-827944dc7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2Tokenizer, Qwen2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb35d18-f1f8-4777-9f9b-2424899234db",
   "metadata": {},
   "source": [
    "## 5. Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4b93ac5d-0ea5-45b2-9a56-5ece2dcfd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define minimum edit distance algorithm via dynamic programming\n",
    "def minimum_edit_distance(source, target):\n",
    "    n = len(source)\n",
    "    m = len(target)\n",
    "    d_mat = np.zeros((n + 1, m + 1))\n",
    "    for i in range(1, n + 1):\n",
    "        d_mat[i, 0] = i\n",
    "    for j in range(1, m + 1):\n",
    "        d_mat[0, j] = j\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            sub = 0 if source[i - 1] == target[j - 1] else 2\n",
    "            del_ = d_mat[i - 1][j] + 1\n",
    "            ins_ = d_mat[i][j - 1] + 1\n",
    "            d_mat[i][j] = min(del_, ins_, d_mat[i - 1][j - 1] + sub)\n",
    "    trace, align_source, align_target = backtrack_alignment(source, target, d_mat)\n",
    "    return d_mat[n, m], trace, align_source, align_target\n",
    "\n",
    "def backtrack_alignment(source, target, d_mat):\n",
    "    align_source, align_target = [], []\n",
    "    i, j = len(source), len(target)\n",
    "    back_trace = [[i, j]]\n",
    "\n",
    "    while (i, j) != (0, 0):\n",
    "        # boundary cases first (avoid negative indexing)\n",
    "        if i == 0:\n",
    "            back_trace.append([i, j - 1])\n",
    "            align_source = [\"*\"] + align_source\n",
    "            align_target = [target[j - 1]] + align_target\n",
    "            j -= 1\n",
    "            continue\n",
    "        if j == 0:\n",
    "            back_trace.append([i - 1, j])\n",
    "            align_source = [source[i - 1]] + align_source\n",
    "            align_target = [\"*\"] + align_target\n",
    "            i -= 1\n",
    "            continue\n",
    "\n",
    "        sub_cost = 0 if source[i - 1] == target[j - 1] else 2\n",
    "\n",
    "        # prefer substitution/match when optimal (your tie-break rule)\n",
    "        if d_mat[i, j] == d_mat[i - 1, j - 1] + sub_cost:\n",
    "            back_trace.append([i - 1, j - 1])\n",
    "            align_source = [source[i - 1]] + align_source\n",
    "            align_target = [target[j - 1]] + align_target\n",
    "            i, j = i - 1, j - 1\n",
    "\n",
    "        # deletion\n",
    "        elif d_mat[i, j] == d_mat[i - 1, j] + 1:\n",
    "            back_trace.append([i - 1, j])\n",
    "            align_source = [source[i - 1]] + align_source\n",
    "            align_target = [\"*\"] + align_target\n",
    "            i -= 1\n",
    "\n",
    "        # insertion\n",
    "        else:\n",
    "            back_trace.append([i, j - 1])\n",
    "            align_source = [\"*\"] + align_source\n",
    "            align_target = [target[j - 1]] + align_target\n",
    "            j -= 1\n",
    "\n",
    "    return back_trace, align_source, align_target\n",
    "\n",
    "# test the minimum edit distance\n",
    "def test_med(source, target):\n",
    "    med, trace, align_source, align_target = minimum_edit_distance(source, target)\n",
    "    print(f\"input source: {source} and target: {target}\")\n",
    "    print(f\"med: {med}\")\n",
    "    print(f\"trace: {trace}\")\n",
    "    print(f\"aligned source: {align_source}\")\n",
    "    print(f\"aligned target: {align_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "89b108a5-bd4a-497f-9a6b-e8fd35bce6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input source: INTENTION and target: EXECUTION\n",
      "med: 8.0\n",
      "trace: [[9, 9], [8, 8], [7, 7], [6, 6], [5, 5], [4, 4], [4, 3], [3, 2], [2, 1], [1, 0], [0, 0]]\n",
      "aligned source: ['I', 'N', 'T', 'E', '*', 'N', 'T', 'I', 'O', 'N']\n",
      "aligned target: ['*', 'E', 'X', 'E', 'C', 'U', 'T', 'I', 'O', 'N']\n"
     ]
    }
   ],
   "source": [
    "test_med(source=\"INTENTION\", target=\"EXECUTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b454ca67-0981-4464-9f7c-11703060b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input source: AGGCTATCACCTGACCTCCAGGCCGATGCCC and target: TAGCTATCACGACCGCGGTCGATTTGCCCGAC\n",
      "med: 15.0\n",
      "trace: [[31, 32], [30, 31], [30, 30], [30, 29], [29, 28], [28, 27], [28, 26], [27, 25], [26, 24], [26, 23], [26, 22], [25, 21], [24, 20], [23, 19], [22, 18], [21, 17], [20, 16], [19, 16], [18, 15], [17, 14], [16, 14], [15, 13], [14, 12], [13, 11], [12, 10], [11, 10], [10, 9], [9, 9], [8, 8], [7, 7], [6, 6], [5, 5], [4, 4], [3, 3], [2, 2], [1, 2], [0, 1], [0, 0]]\n",
      "aligned source: ['*', 'A', 'G', 'G', 'C', 'T', 'A', 'T', 'C', 'A', 'C', 'C', 'T', 'G', 'A', 'C', 'C', 'T', 'C', 'C', 'A', 'G', 'G', 'C', 'C', 'G', 'A', '*', '*', 'T', 'G', '*', 'C', 'C', '*', '*', 'C']\n",
      "aligned target: ['T', 'A', '*', 'G', 'C', 'T', 'A', 'T', 'C', 'A', '*', 'C', '*', 'G', 'A', 'C', 'C', '*', 'G', 'C', '*', 'G', 'G', 'T', 'C', 'G', 'A', 'T', 'T', 'T', 'G', 'C', 'C', 'C', 'G', 'A', 'C']\n"
     ]
    }
   ],
   "source": [
    "test_med(source=\"AGGCTATCACCTGACCTCCAGGCCGATGCCC\", target=\"TAGCTATCACGACCGCGGTCGATTTGCCCGAC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
