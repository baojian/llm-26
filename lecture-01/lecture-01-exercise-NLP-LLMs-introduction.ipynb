{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c6cb6-e001-4bda-b191-0b6b727340fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "/* JupyterLab: center notebook and leave margins */\n",
    ".jp-NotebookPanel-notebook {\n",
    "  width: 85% !important;              \n",
    "  margin-left: auto !important;\n",
    "  margin-right: auto !important;\n",
    "  max-width: 1100px !important;       /* optional cap */\n",
    "}\n",
    "\n",
    "/* Make output area match nicely */\n",
    ".jp-OutputArea {\n",
    "  max-width: 100% !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a9255-01ee-4c70-9ef3-c9a5d00e46c5",
   "metadata": {},
   "source": [
    "## 0. Preparing Your Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc4a81-87a4-4433-84fd-4bef8b1860fb",
   "metadata": {},
   "source": [
    "### 0.1 Install Anaconda and your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d74158c-58bb-46a0-a532-dc964d8e0b3a",
   "metadata": {},
   "source": [
    "**If you are using Windows, I strongly recommend installing Ubuntu 22.04 via [WSL](https://ubuntu.com/desktop/wsl).** If you are using macOS or Linux, you are ready to go.\n",
    "\n",
    "\n",
    "- **Step 1: Install Anaconda.** Download and install Anaconda from: [https://www.anaconda.com/download](https://www.anaconda.com/download).\n",
    "\n",
    "- **Step 2: Create and activate the course environment.** Install your conda environment\n",
    "\n",
    "  - If you have GPU in your machine (Linux + NVIDIA GPU), please create your env via\n",
    "\n",
    "    ```conda env create -f environment-gpu.yml```\n",
    "\n",
    "    Activate it with:\n",
    "\n",
    "    ```conda activate llm-26-gpu```\n",
    "\n",
    "    Note: environment-gpu.yml uses ```pytorch-cuda=12.1```. If your GPU driver/CUDA setup does not support CUDA 12.1, change this version accordingly (e.g., 11.8).\n",
    "\n",
    "  - If you use macOS / Windows (WSL) / Linux CPU-only, please create your env via\n",
    " \n",
    "    ```conda env create -f environment.yml```\n",
    " \n",
    "    Activate it with:\n",
    " \n",
    "    ```conda activate llm-26```\n",
    "\n",
    "- Note, if you got errors when installing ```en_core_web_sm``` or ```zh_core_web_sm```, please remove these two from yml file and install separately in your env via\n",
    "\n",
    "  ```shell\n",
    "  conda activate llm-26 # or conda activate llm-26-gpu\n",
    "  python -m spacy download en_core_web_sm\n",
    "  python -m spacy download zh_core_web_sm\n",
    "  ```\n",
    "  After the above steps, you are ready to download our course github project. Make sure you have [git](https://git-scm.com/install/) in your system.\n",
    "\n",
    "  ```git clone git@github.com:baojian/llm-26.git```\n",
    "\n",
    "  After the above steps, you are ready to open our jupyter notebook.\n",
    "\n",
    "- **Step 3: Open your jupyter notebook.**\n",
    "\n",
    "  All your code will run on Jupyter notebook, you can activate jupyter notebook, via\n",
    "\n",
    "  ```conda activate llm-26 # or conda activate llm-26-gpu```\n",
    "  \n",
    "  ```jupyter lab```\n",
    "\n",
    "  For students using Windows WSL (Ubuntu 22.04), even though Jupyter runs inside WSL (Ubuntu), you can open it directly in the Windows browser via port forwarding (usually automatic). In WSL Ubuntu, start Jupyter like this:\n",
    "\n",
    "  ```jupyter lab --no-browser --ip=127.0.0.1 --port=8888```\n",
    "\n",
    "  It will print a URL like: ```http://127.0.0.1:8888/lab?token=...```\n",
    "\n",
    "  Now on Windows, open your browser and go to: ```http://localhost:8888/lab``` Paste the token if it asks.\n",
    "\n",
    "  ‚úÖ On WSL2, Windows automatically forwards localhost:8888 to the WSL instance in most setups. If localhost:8888 doesn‚Äôt work, in WSL run: ```hostname -I```, suppose it prints something like 172.27.123.45 ..., then open in Windows: ```http://172.27.123.45:8888/lab```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08a6ba-7ca1-42cd-b3bd-003e031584cc",
   "metadata": {},
   "source": [
    "### 0.2 Checking your device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e08efc-8851-4212-8773-5fc8de4f9065",
   "metadata": {},
   "source": [
    "After install conda and your env, you can check whether these packages are installed in the right way.\n",
    "```shell\n",
    "python -c \"import torch; print('torch', torch.__version__); print('cuda?', torch.cuda.is_available()); print('mps?', getattr(torch.backends,'mps',None) is not None and torch.backends.mps.is_available()); print('cuda device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'no gpu'); print('using:', 'cuda' if torch.cuda.is_available() else ('mps' if (getattr(torch.backends,'mps',None) is not None and torch.backends.mps.is_available()) else 'cpu'))\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3168cf2-8767-451f-9286-7224bc2042a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.9.1\n",
      "cuda? False\n",
      "mps? True\n",
      "cuda device: no gpu\n",
      "using: mps\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print('torch', torch.__version__); print('cuda?', torch.cuda.is_available()); print('mps?', getattr(torch.backends,'mps',None) is not None and torch.backends.mps.is_available()); print('cuda device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'no gpu'); print('using:', 'cuda' if torch.cuda.is_available() else ('mps' if (getattr(torch.backends,'mps',None) is not None and torch.backends.mps.is_available()) else 'cpu'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d99b1f6-d442-4680-aefd-5292a40333c7",
   "metadata": {},
   "source": [
    "- The following code provides a more symmetric way to perform the same checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95e4a5c6-5554-4f5a-942b-538abdd19fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1\n",
      "device: mps\n",
      "mps available: True (Apple Metal)\n",
      "device: mps:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def detect_torch_device(verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Returns one of: 'cuda', 'mps', 'cpu'\n",
    "    Priority: CUDA GPU > Apple MPS > CPU\n",
    "    \"\"\"\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    has_mps = getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available()\n",
    "\n",
    "    if has_cuda:\n",
    "        device = \"cuda\"\n",
    "    elif has_mps:\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"torch: {torch.__version__}\")\n",
    "        print(f\"device: {device}\")\n",
    "\n",
    "        if has_cuda:\n",
    "            print(f\"cuda devices: {torch.cuda.device_count()}\")\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                print(f\"  [{i}] {torch.cuda.get_device_name(i)}\")\n",
    "        elif has_mps:\n",
    "            print(\"mps available: True (Apple Metal)\")\n",
    "        else:\n",
    "            print(\"cpu only\")\n",
    "\n",
    "    return device\n",
    "\n",
    "device = detect_torch_device()\n",
    "# generate 2x3 random matrix to check torch\n",
    "x = torch.rand(2, 3)\n",
    "x = x.to(device)\n",
    "print(\"device:\", x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a766f-ef33-42c2-bb12-65b4127af6fd",
   "metadata": {},
   "source": [
    "During our lectures, some datasets are very large. To make sure you have enough disk space, you can check your disk, memory and cpu information via the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77cb9722-9c05-4ce7-a9c3-3155106bc6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== System Report ===\n",
      "OS: Darwin 24.3.0\n",
      "Platform: macOS-15.3.1-arm64-arm-64bit\n",
      "Python: 3.12.9\n",
      "\n",
      "--- CPU ---\n",
      "CPU cores (logical): 12\n",
      "\n",
      "--- Memory (RAM) ---\n",
      "Total: 18.00 GB\n",
      "Available: 2.88 GB\n",
      "Used: 7.29 GB (84.0%)\n",
      "\n",
      "--- Disk ---\n",
      "Path checked: /Users/baojianzhou/git/llm-26/lecture-01\n",
      "Total: 926.35 GB\n",
      "Free:  320.82 GB\n",
      "Used:  605.53 GB\n",
      "\n",
      "--- PyTorch Device ---\n",
      "torch: 2.9.1\n",
      "CUDA available: False\n",
      "MPS available: True\n",
      "Suggested device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import shutil\n",
    "\n",
    "def bytes_to_gb(x: int) -> float:\n",
    "    return x / (1024 ** 3)\n",
    "\n",
    "def system_report(path: str = \".\"):\n",
    "    print(\"=== System Report ===\")\n",
    "    print(\"OS:\", platform.system(), platform.release())\n",
    "    print(\"Platform:\", platform.platform())\n",
    "    print(\"Python:\", platform.python_version())\n",
    "\n",
    "    # CPU\n",
    "    print(\"\\n--- CPU ---\")\n",
    "    print(\"CPU cores (logical):\", os.cpu_count())\n",
    "\n",
    "    # Memory (best-effort, cross-platform)\n",
    "    print(\"\\n--- Memory (RAM) ---\")\n",
    "    try:\n",
    "        import psutil  # you already have this in env\n",
    "        vm = psutil.virtual_memory()\n",
    "        print(f\"Total: {bytes_to_gb(vm.total):.2f} GB\")\n",
    "        print(f\"Available: {bytes_to_gb(vm.available):.2f} GB\")\n",
    "        print(f\"Used: {bytes_to_gb(vm.used):.2f} GB ({vm.percent}%)\")\n",
    "    except Exception as e:\n",
    "        print(\"psutil not available or failed:\", e)\n",
    "\n",
    "    # Disk\n",
    "    print(\"\\n--- Disk ---\")\n",
    "    total, used, free = shutil.disk_usage(path)\n",
    "    print(\"Path checked:\", os.path.abspath(path))\n",
    "    print(f\"Total: {bytes_to_gb(total):.2f} GB\")\n",
    "    print(f\"Free:  {bytes_to_gb(free):.2f} GB\")\n",
    "    print(f\"Used:  {bytes_to_gb(used):.2f} GB\")\n",
    "\n",
    "    # PyTorch device\n",
    "    print(\"\\n--- PyTorch Device ---\")\n",
    "    try:\n",
    "        import torch\n",
    "        cuda = torch.cuda.is_available()\n",
    "        mps = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "        print(\"torch:\", torch.__version__)\n",
    "        print(\"CUDA available:\", cuda)\n",
    "        print(\"MPS available:\", mps)\n",
    "        if cuda:\n",
    "            print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "        device = \"cuda\" if cuda else (\"mps\" if mps else \"cpu\")\n",
    "        print(\"Suggested device:\", device)\n",
    "    except Exception as e:\n",
    "        print(\"torch not available or failed:\", e)\n",
    "\n",
    "system_report(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c006fe-ca20-452d-8263-e23a2bf29d1e",
   "metadata": {},
   "source": [
    "## 1. Explore LLMs via ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d170128b-66ac-4b06-8671-0ce878a617ea",
   "metadata": {},
   "source": [
    "### 1.1  Install ollama and download Qwen3:0.6b and Qwen3:1.7b\n",
    "\n",
    "We can download some popular LLMs such as Qwen series.\n",
    "\n",
    "- Download ollama to your laptop from [https://ollama.com/download](https://ollama.com/download)\n",
    "- Run ```shell ollama run qwen3:0.6b ```, it will automatically download the model into your local disk. It takes about 498MB disk space.\n",
    "- Similarly, you can run, ```shell ollama run qwen3:1.7b ```. It takes about 1.3GB disk space.\n",
    "\n",
    "You can install the python api of ollama via the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110bf6b-064d-4c74-9f96-94e4fc702b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::ollama -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756eae67-2082-4131-bc2b-17b95b04bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge ollama-python -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cfbce0-e58e-4558-8291-4536201feebb",
   "metadata": {},
   "source": [
    "### 1.2 Get response for a given prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1791de-ae7b-40bf-9c38-e175a0527044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import ollama\n",
    "\n",
    "# If you have proxy, make sure bypass proxies for local services (e.g., Ollama on localhost:11434)\n",
    "# You can check whether Ollama is running or not by tpying: http://127.0.0.1:11434/ in your Chrome.\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1\"\n",
    "os.environ[\"no_proxy\"] = \"localhost,127.0.0.1\"\n",
    "\n",
    "models = [\"qwen3:0.6b\", \"qwen3:1.7b\"]\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "\n",
    "for model in models:\n",
    "    print('-' * 50)\n",
    "    start_time = time.time()\n",
    "    for _ in range(10):\n",
    "        resp = ollama.generate(\n",
    "            model = model,\n",
    "            prompt = prompt\n",
    "        )\n",
    "        print(f\"{model} with resp {_ + 1}: {resp[\"response\"]}\")\n",
    "    print(f'total runtime of 10 responses of {model} is: {time.time() - start_time:.1f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b74f3-b395-4c63-8d2f-d71d9adb532f",
   "metadata": {},
   "source": [
    "**Some key observations:**\n",
    "\n",
    "- The smaller model Qwen3:0.6b produces lower quality answers while the relative bigger model Qwen3:1.7b produces high quality answers.\n",
    "- The response is a kind of random as each time the response may not be the same.\n",
    "\n",
    "Certainly, there are ways to make sure the above to generate a fixed answer. For example, you can use the following code where each time it always produce the maximal probability as the answer:\n",
    "\n",
    "```python\n",
    "resp = ollama.generate(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    options={\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 0,\n",
    "        # optional:\n",
    "        \"num_predict\": 32,\n",
    "    },\n",
    ")\n",
    "print(resp[\"response\"])\n",
    "```\n",
    "\n",
    "Let us generate some response that are not one word but a sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3446ef-8081-45f8-8852-da387bb19b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"qwen3:1.7b\"\n",
    "prompt = \"I am an undergraduate student, please explain LLMs in three sentences.\"\n",
    "resp = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt\n",
    "        )\n",
    "print(f\"Prompt: {prompt} \\nResp: {resp[\"response\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b2927-4bff-43b1-bb37-e11563bf44a4",
   "metadata": {},
   "source": [
    "### 1.3 Get response probability from LLMs\n",
    "\n",
    "From above outputs, you see that each time, the response of these LLMs could be different. Actually, all LLMs are probabilitic model where each time, they response (i.e., answers) prompts (i.e., questions) differently. We can use math language to describe this inference process.\n",
    "\n",
    "Let $p_\\theta(\\cdot)$ be the trained probability model. Here, you can think $p_\\theta(\\cdot)$ as Qwen3:0.6b, Qwen3:1.7b or any other models. Given the prompt *Fudan University is located in which city? Answer with one word.* (a sequence of tokens), the above response will give you an answer, which is also a sequence of tokens. So, the model will use an algorithm to predict the response given the prompt. Use the math language, we can think it tries to calculate the following probability:\n",
    "\n",
    "$$\n",
    "p_\\theta \\left(w_{n+1}|w_1,w_2,\\ldots, w_{n}\\right),\n",
    "$$\n",
    "where\n",
    "- Prompt=[*Fudan University is located in which city? Answer with one word.]* $=[w_1,w_2,\\ldots, w_{n}]$,\n",
    "- Response=[$w_{n+1}$].\n",
    "\n",
    "The model will try to output the most likeliy word $w_{n+1}$ using its own *inference algorithm*. We will detail this part in later lectures. By the definition of conditional probability, we may calculate it as\n",
    "\n",
    "$$\n",
    "p_\\theta \\left(w_{n+1}|w_1,w_2,\\ldots, w_{n}\\right) = \\frac{ p_\\theta \\left(w_1,w_2,\\ldots, w_{n}, w_{n+1}\\right)}{p_\\theta \\left(w_1,w_2,\\ldots, w_{n}\\right)}.\n",
    "$$\n",
    "\n",
    "The above is essential to say that, if we can learn a model that can represent any length sequence of distribution $p_\\theta \\left(w_1,w_2,\\ldots, w_{k}\\right)$, where $k$ could be any positive integer, then we can compute the conditional distribution easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f2408-7a79-4980-81c3-fa96f5059598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"qwen3:0.6b\" # \"qwen3:1.7b\"\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "num_top_tokens = 20 # number of top alternatives per generated token\n",
    "resp = ollama.generate(\n",
    "    model = model,\n",
    "    prompt = prompt,\n",
    "    stream = False,\n",
    "    logprobs = True,\n",
    "    think = False,\n",
    "    top_logprobs = num_top_tokens\n",
    ")\n",
    "print(\"response:\", repr(resp[\"response\"]))\n",
    "\n",
    "# Each element usually corresponds to one generated token\n",
    "for i, lp in enumerate(resp.get(\"logprobs\", [])):\n",
    "    tok = lp.get(\"token\")\n",
    "    logp = lp.get(\"logprob\")\n",
    "    p = math.exp(logp) if logp is not None else None\n",
    "    print(f\"{i:02d} token={tok!r:>16} logp={logp: .4f} p={p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf85685-d462-4406-a7c5-c65de6f220d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, ollama\n",
    "\n",
    "model = \"qwen3:0.6b\"\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "\n",
    "res = ollama.generate(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    logprobs=True,\n",
    "    think = False,\n",
    "    top_logprobs=10,\n",
    "    options={\"temperature\": 0.0, # greedy decoding, it pick the maximal token\n",
    "             \"num_predict\": 20,\n",
    "            \"think\": False # do not use thinking model.\n",
    "            },\n",
    ")\n",
    "\n",
    "answer = ''\n",
    "lp = res[\"logprobs\"]\n",
    "tokens = [d.get(\"token\", \"\") for d in lp]\n",
    "print(f'We use model: {model}')\n",
    "for i in range(len(lp)):\n",
    "    tok = lp[i].get(\"token\", \"\")\n",
    "    logp = lp[i].get(\"logprob\", None)\n",
    "    alts = lp[i].get(\"top_logprobs\", [])\n",
    "    p = math.exp(logp) if logp is not None else float(\"nan\")\n",
    "    if tok == \"\\n\" or tok == \"\\n\\n\": # stop when answer ends (often newline).\n",
    "        break\n",
    "    answer += tok\n",
    "    print(f\"--- top probabilities of token-{i:02d} ---\")\n",
    "    for a in alts[:20]:\n",
    "        prob_a = math.exp(a['logprob'])\n",
    "        print(f\"{a['token']!r:>12}:{prob_a:.5f}\")\n",
    "    print(f\"Partial Response: {answer}\\n\")\n",
    "print(f\"Final Response: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02071500-6ccb-4bcb-89d7-d01f2e7322ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"qwen3:1.7b\"\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "\n",
    "res = ollama.generate(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    logprobs=True,\n",
    "    think = False,\n",
    "    top_logprobs=10,\n",
    "    options={\"temperature\": 0.0, # greedy decoding, it pick the maximal token\n",
    "             \"num_predict\": 20,\n",
    "            \"think\": False # do not use thinking model.\n",
    "            },\n",
    ")\n",
    "print(res[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444aad1c-b734-480c-9db4-419682decc26",
   "metadata": {},
   "source": [
    "If we do not use thinking mode, it will gives the above response. However, we can still see how Shanghai is chosen during the inference stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe4519-17fe-42f8-a2d7-dd419a3cdf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, ollama\n",
    "\n",
    "model = \"qwen3:1.7b\"\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "\n",
    "res = ollama.generate(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    logprobs=True,\n",
    "    think = False, # Do not use the thinking model.\n",
    "    top_logprobs=10,\n",
    "    options={\"temperature\": 0.0, # greedy decoding, it pick the maximal token\n",
    "             \"num_predict\": 20,\n",
    "            \"think\": False # do not use thinking model.\n",
    "            },\n",
    ")\n",
    "\n",
    "answer = ''\n",
    "lp = res[\"logprobs\"]\n",
    "tokens = [d.get(\"token\", \"\") for d in lp]\n",
    "print(f'We use model: {model}')\n",
    "for i in range(len(lp)):\n",
    "    tok = lp[i].get(\"token\", \"\")\n",
    "    logp = lp[i].get(\"logprob\", None)\n",
    "    alts = lp[i].get(\"top_logprobs\", [])\n",
    "    p = math.exp(logp) if logp is not None else float(\"nan\")\n",
    "    if tok == \"\\n\" or tok == \"\\n\\n\": # stop when answer ends (often newline).\n",
    "        break\n",
    "    answer += tok\n",
    "    print(f\"--- top probabilities of token-{i:02d} ---\")\n",
    "    for a in alts[:20]:\n",
    "        prob_a = math.exp(a['logprob'])\n",
    "        print(f\"{a['token']!r:>12}:{prob_a:.5f}\")\n",
    "    print(f\"Partial Response: {answer}\\n\")\n",
    "print(f\"Final Response: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21141a2-a2d4-457c-a519-c231c0fcf0cf",
   "metadata": {},
   "source": [
    "## 2. Datasets Exploration\n",
    "\n",
    "Please check this part in our slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d7dc7-3fd7-4fec-9736-5b6ae7a6dd56",
   "metadata": {},
   "source": [
    "### 2.1 Explore wikitext-2 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d17f6b-b63e-43bf-846e-4709ec4e86fc",
   "metadata": {},
   "source": [
    "Please note that huggingface cannot be directly used in China. An alternative way is to use [https://hf-mirror.com/](https://hf-mirror.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1610a-18de-4641-823f-2e6c5c2de910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# If this does not work, please add export HF_ENDPOINT=https://hf-mirror.com in your env.\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HUB_ETAG_TIMEOUT\"] = \"60\"\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"60\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# loads train/validation/test\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b64849-cebb-447e-93a0-faaac615afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)\n",
    "train = ds[\"train\"]\n",
    "for i in range(4):\n",
    "    print(train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb8615-e97c-4638-a00d-d7e8452b31d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = ds[\"train\"]\n",
    "\n",
    "# simple word tokenizer (lowercased)\n",
    "word_re = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
    "\n",
    "def heaps_curve(dataset, step=1000):\n",
    "    V = set()\n",
    "    N = 0\n",
    "    Ns, Vs = [], []\n",
    "\n",
    "    for ex in dataset:\n",
    "        text = ex[\"text\"]\n",
    "        if not text or text.strip() == \"\": # empty word -> continue\n",
    "            continue\n",
    "        # make all words to low cases\n",
    "        words = word_re.findall(text.lower()) \n",
    "        for w in words:\n",
    "            N += 1\n",
    "            V.add(w)\n",
    "\n",
    "            if N % step == 0:\n",
    "                Ns.append(N)\n",
    "                Vs.append(len(V))\n",
    "    return np.array(Ns), np.array(Vs)\n",
    "\n",
    "Ns, Vs = heaps_curve(train, step=1000)\n",
    "\n",
    "# Fit log |V| = log k + beta log N  -> linear regression on logs\n",
    "logN = np.log(Ns)\n",
    "logV = np.log(Vs)\n",
    "beta, logk = np.polyfit(logN, logV, 1)\n",
    "k = np.exp(logk)\n",
    "\n",
    "print(f\"Fitted Heaps' law: |V| ‚âà {k:.2f} * N^{beta:.3f}\")\n",
    "\n",
    "# Plot (log-log)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.loglog(Ns, Vs, marker='o', linestyle='none', markersize=5, label=\"Empirical (Wikitext-2)\")\n",
    "\n",
    "# fitted line\n",
    "Ns_line = np.linspace(Ns[0], Ns[-1], 200)\n",
    "Vs_line = k * (Ns_line ** beta)\n",
    "plt.loglog(Ns_line, Vs_line, label=fr\"Fitted ($k={k:.3f},\\beta=${beta:.3f})\")\n",
    "\n",
    "plt.xlabel(\"$N$ (tokens)\", fontsize = 15)\n",
    "plt.ylabel(\"$|V|$ (vocab)\", fontsize = 15)\n",
    "plt.title(r\"Heaps' Law (Simple tokenization): $|V|=kN^{\\beta}$\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7efa3f3-d578-4f15-b5ca-824d847a5c1d",
   "metadata": {},
   "source": [
    "Next, we can use spacy to do the tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7175ad4-3f82-471d-ba00-16b6af403ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"ner\",\"lemmatizer\"])\n",
    "# tokenizer still works even with pipeline disabled\n",
    "\n",
    "train = ds[\"train\"]\n",
    "\n",
    "def heaps_curve_spacy(dataset, step=10_000, batch_size=256):\n",
    "    V = set()\n",
    "    N = 0\n",
    "    Ns, Vs = [], []\n",
    "\n",
    "    texts = (ex[\"text\"] for ex in dataset if ex[\"text\"] and ex[\"text\"].strip())\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        for tok in doc:\n",
    "            # choose your definition of \"word\"\n",
    "            if tok.is_alpha:\n",
    "                w = tok.text.lower()\n",
    "                N += 1\n",
    "                V.add(w)\n",
    "                if N % step == 0:\n",
    "                    Ns.append(N)\n",
    "                    Vs.append(len(V))\n",
    "    return np.array(Ns), np.array(Vs)\n",
    "\n",
    "Ns, Vs = heaps_curve_spacy(train, step=10_000)\n",
    "\n",
    "# Fit log |V| = log k + beta log N\n",
    "logN = np.log(Ns)\n",
    "logV = np.log(Vs)\n",
    "beta, logk = np.polyfit(logN, logV, 1)\n",
    "k = np.exp(logk)\n",
    "\n",
    "print(f\"Fitted Heaps' law (spaCy): |V| ‚âà {k:.2f} * N^{beta:.3f}\")\n",
    "# Plot (log-log)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.loglog(Ns, Vs, marker='o', linestyle='none', markersize=5, label=\"Empirical (Wikitext-2)\")\n",
    "\n",
    "# fitted line\n",
    "Ns_line = np.linspace(Ns[0], Ns[-1], 200)\n",
    "Vs_line = k * (Ns_line ** beta)\n",
    "plt.loglog(Ns_line, Vs_line, label=fr\"Fitted ($k={k:.3f},\\beta=${beta:.3f})\")\n",
    "\n",
    "plt.xlabel(\"$N$ (tokens)\", fontsize = 15)\n",
    "plt.ylabel(\"$|V|$ (vocab)\", fontsize = 15)\n",
    "plt.title(r\"Heaps' Law (Spacy tokenization): $|V|=kN^{\\beta}$\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c1be1c-9383-4eea-b01a-89d51feddfe4",
   "metadata": {},
   "source": [
    "### 2.2 Explore wikitext-103 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830f927-3163-42ad-8014-ebacf01445ae",
   "metadata": {},
   "source": [
    "We can download a larger dataset from the following:\n",
    "\n",
    "```shell\n",
    "export HF_HOME=$HOME/.cache/huggingface\n",
    "\n",
    "huggingface-cli download Salesforce/wikitext --repo-type dataset --resume-download --include \"wikitext-103-raw-v1/*.parquet\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607e4bd-5b60-4d40-8fbb-a24ae45e1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")  # will hit cache if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b8d21-cf02-41b2-92d3-eb49597fc399",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)\n",
    "train = ds[\"train\"]\n",
    "for i in range(4):\n",
    "    print(train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9da9b-9328-4c34-8e7a-b61d35d07722",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds[\"train\"]\n",
    "# simple word tokenizer (lowercased)\n",
    "word_re = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
    "\n",
    "def heaps_curve(dataset, step=1000):\n",
    "    V = set()\n",
    "    N = 0\n",
    "    Ns, Vs = [], []\n",
    "\n",
    "    for ex in dataset:\n",
    "        text = ex[\"text\"]\n",
    "        if not text or text.strip() == \"\": # empty word -> continue\n",
    "            continue\n",
    "        # make all words to low cases\n",
    "        words = word_re.findall(text.lower()) \n",
    "        for w in words:\n",
    "            N += 1\n",
    "            V.add(w)\n",
    "\n",
    "            if N % step == 0:\n",
    "                Ns.append(N)\n",
    "                Vs.append(len(V))\n",
    "    return np.array(Ns), np.array(Vs)\n",
    "\n",
    "Ns, Vs = heaps_curve(train, step=1000)\n",
    "\n",
    "# Fit log |V| = log k + beta log N  -> linear regression on logs\n",
    "logN = np.log(Ns)\n",
    "logV = np.log(Vs)\n",
    "beta, logk = np.polyfit(logN, logV, 1)\n",
    "k = np.exp(logk)\n",
    "\n",
    "print(f\"Fitted Heaps' law (spaCy): |V| ‚âà {k:.2f} * N^{beta:.3f}\")\n",
    "# Plot (log-log)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.loglog(Ns, Vs, marker='o', linestyle='none', markersize=5, label=\"Empirical (Wikitext-103)\")\n",
    "\n",
    "# fitted line\n",
    "Ns_line = np.linspace(Ns[0], Ns[-1], 200)\n",
    "Vs_line = k * (Ns_line ** beta)\n",
    "plt.loglog(Ns_line, Vs_line, label=fr\"Fitted ($k={k:.3f},\\beta=${beta:.3f})\")\n",
    "\n",
    "plt.xlabel(\"$N$ (tokens)\", fontsize = 15)\n",
    "plt.ylabel(\"$|V|$ (vocab)\", fontsize = 15)\n",
    "plt.title(r\"Heaps' Law (Spacy tokenization): $|V|=kN^{\\beta}$\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99541743-cc2c-4dad-b46f-0760d97d44e2",
   "metadata": {},
   "source": [
    "### 2.3 BookCorpus datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00b8244-73d0-4406-a592-02bb3ad6281c",
   "metadata": {},
   "source": [
    "BookCorpus dataset has been used in training GPT-1. See <a href=\"/files/papers/GPT1-Improving_Language_Understanding_by_Generative_Pre-Training.pdf\"\n",
    "   target=\"_blank\" rel=\"noopener\">\n",
    "  GPT-1 paper (PDF)\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4ad7c-3b3e-4e12-aac2-a2e589889b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tarfile\n",
    "\n",
    "path = \"../.datasets/books1.tar.gz\"\n",
    "start_time = time.time()\n",
    "with tarfile.open(path, \"r:gz\") as tar:\n",
    "    names = tar.getnames()\n",
    "    print(\"num members:\", len(names))\n",
    "    print(\"\\n\".join(names[:50]))\n",
    "print(f\"total time to scan BookCorpus: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5979d39-07ea-40a0-a9bd-0087d21918f0",
   "metadata": {},
   "source": [
    "You can check ```shell tokenization_bookcorpus.py``` to generate heaps statistics in jsonl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d353d-6016-47b8-b5e1-426dbf1a9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "def load_heaps_jsonl(log_path: str):\n",
    "    Ns, Vs = [], []\n",
    "    with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            Ns.append(r[\"total_tokens\"])\n",
    "            Vs.append(r[\"vocab_size\"])\n",
    "    return np.array(Ns), np.array(Vs)\n",
    "\n",
    "Ns, Vs = load_heaps_jsonl(\"books1_wordcounts.heaps_log.jsonl\")\n",
    "\n",
    "# fit log |V| = log k + beta log N\n",
    "beta, logk = np.polyfit(np.log(Ns), np.log(Vs), 1)\n",
    "k = np.exp(logk)\n",
    "print(f\"Fit: |V| ‚âà {k:.2f} * N^{beta:.3f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.loglog(Ns, Vs, marker=\"o\", linestyle=\"none\", \n",
    "           markersize=5, label=\"Empirical (BookCorpus)\")\n",
    "Ns_line = np.linspace(Ns[0], Ns[-1], 200)\n",
    "plt.loglog(Ns_line, k * (Ns_line ** beta), label=fr\"Fitted Curve $(k={k:.3f},\\beta=${beta:.3f})\")\n",
    "plt.xlabel(\"$N$ (tokens)\", fontsize = 14)\n",
    "plt.ylabel(\"$|V|$ (vocab)\", fontsize = 14)\n",
    "plt.title(r\"Heaps' Law (Simple tokenization): $|V|=k N^{\\beta}$\", fontsize = 14)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08ee8c-34b5-4373-a2cc-9ed8b929066a",
   "metadata": {},
   "source": [
    "## 3. Basics for Python and spaCy\n",
    "\n",
    "- **Python**: We will use Python-3.12 in our course.\n",
    "- **spaCy**: As introduced in [https://github.com/explosion/spaCy](https://github.com/explosion/spaCy), [spaCy](https://spacy.io/) is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. We will use it to demonostrate how to do text tokenization.\n",
    "- **nltk tool**: In our previous courses, we will introduce nltk tool for tokenization stuff. But we will not cover this part in our new course as these tools are largely iirelvant and outdated. One can find details of nltk at [https://github.com/nltk/nltk](https://github.com/nltk/nltk) and website at [https://www.nltk.org/](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301e767-854c-43de-a7c6-6e1e3528ddcd",
   "metadata": {},
   "source": [
    "### 3.1 Python basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f651c-86f1-4274-ac7f-2ef1926d019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python, there is a built in lib re, we can import them\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a56fac-f9f2-470e-a706-4fea0a6091fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Find woodchuck or Woodchuck : Disjunction\n",
    "test_str = \"This string contains Woodchuck and woodchuck.\"\n",
    "result=re.search(pattern=\"[wW]oodchuck\", string=test_str)\n",
    "print(result)\n",
    "result=re.search(pattern=r\"[wW]ooodchuck\", string=test_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1e739-90fc-41a7-8781-f35b5ce223af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the word \"woodchuck\" in the following test string\n",
    "test_str = \"interesting links to woodchucks ! and lemurs!\"\n",
    "re.search(pattern=\"woodchuck\", string=test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb804d-bdc0-471c-8606-bb178bfa3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find !, it follows the same way:\n",
    "print(re.search(pattern=\"!\", string=test_str))\n",
    "print(re.search(pattern=\"!!\", string=test_str))\n",
    "assert re.search(pattern=\"!!\", string=test_str) == None # match nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17845520-c90c-4a20-bbd5-8304b983f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find any single digit in a string.\n",
    "result=re.search(pattern=r\"[0123456789]\", string=\"plenty of 7 to 5\")\n",
    "print(result)\n",
    "result=re.search(pattern=r\"[0-9]\", string=\"plenty of 7 to 5\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896eb05-f93d-475c-a9de-30f20117457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negation: If the caret ^ is the first symbol after [,\n",
    "# the resulting pattern is negated. For example, the pattern \n",
    "# [^a] matches any single character (including special characters) except a.\n",
    "\n",
    "# -- not an upper case letter\n",
    "print(re.search(pattern=r\"[^A-Z]\", string=\"Oyfn pripetchik\"))\n",
    "\n",
    "# -- neither 'S' nor 's'\n",
    "print(re.search(pattern=r\"[^Ss]\", string=\"I have no exquisite reason for't\"))\n",
    "\n",
    "# -- not a period\n",
    "print(re.search(pattern=r\"[^.]\", string=\"our resident Djinn\"))\n",
    "\n",
    "# -- either 'e' or '^'\n",
    "print(re.search(pattern=r\"[e^]\", string=\"look up ^ now\"))\n",
    "\n",
    "# -- the pattern ‚Äòa^b‚Äô\n",
    "print(re.search(pattern=r'a\\^b', string=r'look up a^b now'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86f6ca-4d0f-4bfd-9d58-05a2cb71fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More disjuncations\n",
    "str1 = \"Woodchucks is another name for groundhog!\"\n",
    "result = re.search(pattern=\"groundhog|woodchuck\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e33f63-ff31-4c0e-8a3d-fd692acd7715",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Find all woodchuckk Woodchuck Groundhog groundhogxxx!\"\n",
    "result = re.findall(pattern=\"[gG]roundhog|[Ww]oodchuck\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6fba48-2c61-4cfe-9f96-8c9aa1f4475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some special chars\n",
    "\n",
    "# ?: Optional previous char\n",
    "str1 = \"Find all color colour colouur colouuur colouyr\"\n",
    "result = re.findall(pattern=\"colou?r\",string=str1)\n",
    "print(result)\n",
    "\n",
    "# *: 0 or more of previous char\n",
    "str1 = \"Find all color colour colouur colouuur colouyr\"\n",
    "result = re.findall(pattern=\"colou*r\",string=str1)\n",
    "print(result)\n",
    "\n",
    "# +: 1 or more of previous char\n",
    "str1 = \"baa baaa baaaa baaaaa\"\n",
    "result = re.findall(pattern=\"baa+\",string=str1)\n",
    "print(result)\n",
    "# .: any char\n",
    "str1 = \"begin begun begun beg3n\"\n",
    "result = re.findall(pattern=\"beg.n\",string=str1)\n",
    "print(result)\n",
    "str1 = \"The end.\"\n",
    "result = re.findall(pattern=\"\\.$\",string=str1)\n",
    "print(result)\n",
    "str1 = \"The end? The end. #t\"\n",
    "result = re.findall(pattern=\".$\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00dcb9e-09bb-4e8a-af1c-f3b53eca5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all \"the\" in a raw text.\n",
    "text = \"If two sequences in an alignment share a common ancestor, \\\n",
    "mismatches can be interpreted as point mutations and gaps as indels (that \\\n",
    "is, insertion or deletion mutations) introduced in one or both lineages in \\\n",
    "the time since they diverged from one another. In sequence alignments of \\\n",
    "proteins, the degree of similarity between amino acids occupying a \\\n",
    "particular position in the sequence can be interpreted as a rough \\\n",
    "measure of how conserved a particular region or sequence motif is \\\n",
    "among lineages. The absence of substitutions, or the presence of \\\n",
    "only very conservative substitutions (that is, the substitution of \\\n",
    "amino acids whose side chains have similar biochemical properties) in \\\n",
    "a particular region of the sequence, suggest [3] that this region has \\\n",
    "structural or functional importance. Although DNA and RNA nucleotide bases \\\n",
    "are more similar to each other than are amino acids, the conservation of \\\n",
    "base pairs can indicate a similar functional or structural role.\"\n",
    "matches = re.findall(\"[^a-zA-Z][tT]he[^a-zA-Z]\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9444c5-bf35-4947-8128-428fb9a9cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nicer way is to do the following\n",
    "\n",
    "matches = re.findall(r\"\\b[tT]he\\b\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007b2336-eac6-4b58-bdbe-65609fcdca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikilink_re():\n",
    "    \"\"\" This regex is from the following Github:\n",
    "    https://github.com/WikiLinkGraphs/wikidump\n",
    "    \"\"\"\n",
    "    regex_str = r'''(?P<total>(?P<wikilink>\n",
    "        \\[\\[(?P<link>[ÀÜ\\n\\|\\]\\[\\<\\>\\{\\}]{0,256})(?:\\|(?P<anchor>[ÀÜ\\[]*?))?\\]\\])\\w*)\\s?'''\n",
    "    return regex.compile(regex_str, regex.VERBOSE | regex.MULTILINE)\n",
    "\n",
    "# Task: Implement the task shown in Slides 52\n",
    "# You may need to\n",
    "# 1. Download a Wikipedia article xml file\n",
    "# 2. Use RE to extract links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b7609-85dd-47f7-b878-1f99f97c5f07",
   "metadata": {},
   "source": [
    "### 3.2 spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80de04-10da-4216-be12-5f10041cfa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## install spaCy\n",
    "!conda install -c conda-forge spacy -y --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6fd509-59b4-4678-8454-53d12d24d9ad",
   "metadata": {},
   "source": [
    "Open your terminal and activate your llm-26 env, and then run the following to download English LMs.\n",
    "```python\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed8b2f-c161-4574-b1d2-863d97f01b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "prompt = \"Fudan University is located in which city? Answer with one word.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(prompt) # i want to do text preprocessing for our prompt.\n",
    "\n",
    "# Text: The original word text.\n",
    "# Lemma: The base form of the word.\n",
    "# POS: The simple UPOS part-of-speech tag.\n",
    "# Tag: The detailed part-of-speech tag.\n",
    "# Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "# Shape: The word shape ‚Äì capitalization, punctuation, digits.\n",
    "# is alpha: Is the token an alpha character? (whether it consists only of letters from the alphabet (A-Z or a-z))\n",
    "# is stop: Is the token part of a stop list, i.e. the most common words of the language? \n",
    "#         (A stop list (or stopwords list) is a list of commonly used words in a language that \n",
    "#         are usually ignored during natural language processing (NLP) tasks, such as text analysis or machine learning.)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"--- token: {token.text} ---\")\n",
    "    print(f\"lemma: {token.lemma_}\\npos: {token.pos_}\\ntag: {token.tag_}\\ndep: {token.dep_}\\nshape: {token.shape_}\\nis_alpha:{token.is_alpha}\\nis_stop: {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c357e-284f-4c1f-9e7d-08c64584249f",
   "metadata": {},
   "source": [
    "There are two type of tokenizations\n",
    "\n",
    "- **Top-down tokenization**: We define a standard and implement rules to implement that kind of tokenization.\n",
    "  - word tokenization\n",
    "  - charater tokenization\n",
    "- **Bottom-up tokenization**: We use simple statistics of letter sequences to break up words into subword tokens.\n",
    "  - subword tokenization (modern LLMs use this type!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436fb389-51e6-4bc6-afab-473dd00798ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use split method via the whitespace \" \"\n",
    "text = \"\"\"While the Unix command sequence just removed all the numbers and punctuation\"\"\"\n",
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b4e05-d517-46d4-9c98-59a40a1d242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But, we have punctuations, icons, and many other small issues.\n",
    "text = \"\"\"Don't you love ü§ó Transformers? We sure do.\"\"\"\n",
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9697946-d678-455d-920e-d73e1cb10daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy works much better\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb21874-ac5c-464d-96da-267865244594",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Special characters and numbers will need to be kept in prices ($45.55) and dates (01/02/06); \n",
    "we don‚Äôt want to segment that price into separate tokens of ‚Äú45‚Äù and ‚Äú55‚Äù. And there are URLs (https://www.stanford.edu),\n",
    "Twitter hashtags (#nlproc), or email addresses (someone@cs.colorado.edu).\"\"\"\n",
    "doc = nlp(text)\n",
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ddcb2-2b96-479d-9424-641cd05dfe49",
   "metadata": {},
   "source": [
    "Please install zh tokenization via spaCy: \n",
    "```python\n",
    "python -m spacy download zh_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab54b4-006b-4422-ac60-d796944abb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "text = 'ÂßöÊòéËøõÂÖ•ÊÄªÂÜ≥Ëµõ'\n",
    "doc = nlp(text)\n",
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b583e-4672-4574-bfbe-cff0ed083cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"1Êúà1Êó•ÔºåÂõΩÂä°Èô¢ÂõΩËµÑÂßîÂÖ¨Â∏É‚Äú2025Âπ¥Â∫¶Â§Æ‰ºÅÂçÅÂ§ßÂõΩ‰πãÈáçÂô®‚ÄùÔºå1Êúà2Êó•ÂÖ¨Â∏É‚Äú2025Âπ¥Â∫¶Â§Æ‰ºÅÂçÅÂ§ßË∂ÖÁ∫ßÂ∑•Á®ã‚Äù„ÄÇ‚Äå‚Äå\"\"\"\n",
    "doc = nlp(text)\n",
    "print([str(token) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcb565-de3a-42b8-82bf-f1fdb294129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.zh import Chinese\n",
    "nlp_ch = Chinese()\n",
    "print([*nlp_ch(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24912cb5-b719-458a-80e9-c43dedfdd887",
   "metadata": {},
   "source": [
    "**Lemmatization (ËØçÂΩ¢ËøòÂéü)**\n",
    "\n",
    "- Lemmatization is the task of determining that two words have the same root, despite their surface differences.\n",
    "- **Motivation**: For some NLP situations, we also want two morphologically different forms of a word to behave similarly. For example in web search, someone may type the string woodchucks but a useful system might want to also return pages\n",
    "that mention woodchuck with no s.\n",
    "- **Example 1**: The words am, are, and is have the shared lemma be.\n",
    "- **Example 2**: The words dinner and dinners both have the lemma dinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107c736-b9ab-4116-8f21-803da85ef2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The Brown Corpus, a text corpus of American English that was compiled in the 1960s at Brown University, \\\n",
    "is widely used in the field of linguistics and natural language processing. It contains about 1 million \\\n",
    "words (or \"tokens\") across a diverse range of texts from 500 sources, categorized into 15 genres, such \\\n",
    "as news, editorial, and fiction, to provide a comprehensive resource for studying the English language. \\\n",
    "This corpus has been instrumental in the development and evaluation of various computational linguistics \\\n",
    "algorithms and tools.\n",
    "\"\"\"\n",
    "text = text.replace(\"\\n\", \" \").strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44864d0-0da0-4ead-80f3-34b47e570a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "print(doc[0], type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46080ca6-20b0-4350-942c-64421cddcc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [token.lemma_ for token in doc]\n",
    "for ori,lemma in zip(doc[:30], lemmas[:30]):\n",
    "    print(ori, lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8d4fd-fb05-410f-a140-ffda88850934",
   "metadata": {},
   "source": [
    "**Stemming (ËØçÂπ≤ÊèêÂèñ)**\n",
    "\n",
    "The Porter-Stemmer method\n",
    "\n",
    "Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of chopping off words final affixes. This naive version of morphological analysis is called stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651f6e4-65b0-4d6f-8b92-d9b19cad112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"\\\n",
    "This was not the map we found in Billy Bones's chest, but \\\n",
    "an accurate copy, complete in all things-names and heights \\\n",
    "and soundings-with the single exception of the red crosses \\\n",
    "and the written notes.\\\n",
    "\"\"\"   \n",
    "doc = nlp(text)\n",
    "\n",
    "for tok in doc:\n",
    "    if tok.is_alpha:\n",
    "        print(tok.text, tok.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3d84a-e5cf-453e-86a6-cfee5287bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A modern and fast NLP library that includes support for sentence segmentation. \n",
    "# spaCy uses a statistical model to predict sentence boundaries, which can be more accurate \n",
    "# than rule-based approaches for complex texts.\n",
    "# Install via conda: conda install conda-forge::spacy\n",
    "# Install via pip:   pip install -U spacy\n",
    "# Download data: python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Here is a sentence. Here is another one! And the last one.\")\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a694a9e-f604-4824-a17d-09d7281c4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    " # You need to install it via: python -m spacy download zh_core_web_sm\n",
    "from spacy.lang.zh.examples import sentences \n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "text = \"\"\"\\\n",
    "Â≠óËäÇÂØπÁºñÁ†ÅÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÊï∞ÊçÆÂéãÁº©ÂΩ¢ÂºèÔºåËøôÁßçÊñπÊ≥ïÁî®Êï∞ÊçÆ‰∏≠‰∏çÂ≠òÁöÑ‰∏Ä‰∏™Â≠óËäÇË°®Á§∫ÊúÄÂ∏∏Âá∫Áé∞ÁöÑËøûÁª≠Â≠óËäÇÊï∞ÊçÆ„ÄÇËøôÊ†∑ÁöÑÊõøÊç¢ÈúÄË¶ÅÈáçÂª∫ÂÖ®ÈÉ®ÂéüÂßãÊï∞ÊçÆ„ÄÇÂ≠óËäÇÂØπÁºñÁ†ÅÂÆû‰æã: ÂÅáËÆæÊàë‰ª¨Ë¶ÅÁºñÁ†ÅÊï∞ÊçÆ aaabdaaabac, Â≠óËäÇÂØπ‚Äúaa‚ÄùÂá∫Áé∞Ê¨°Êï∞ÊúÄÂ§öÔºåÊâÄ‰ª•Êàë‰ª¨Áî®Êï∞ÊçÆ‰∏≠Ê≤°ÊúâÂá∫Áé∞ÁöÑÂ≠óËäÇ‚ÄúZ‚ÄùÊõøÊç¢‚Äúaa‚ÄùÂæóÂà∞ÊõøÊç¢Ë°®\n",
    "Z <- aa Êï∞ÊçÆËΩ¨Âèò‰∏∫ ZabdZabac. Âú®Ëøô‰∏™Êï∞ÊçÆ‰∏≠ÔºåÂ≠óËäÇÂØπ‚ÄúZa‚ÄùÂá∫Áé∞ÁöÑÊ¨°Êï∞ÊúÄÂ§öÔºåÊàë‰ª¨Áî®Âè¶Â§ñ‰∏Ä‰∏™Â≠óËäÇ‚ÄúY‚ÄùÊù•ÊõøÊç¢ÂÆÉÔºàËøôÁßçÊÉÖÂÜµ‰∏ãÁî±‰∫éÊâÄÊúâÁöÑ‚ÄúZ‚ÄùÈÉΩÂ∞ÜË¢´ÊõøÊç¢ÔºåÊâÄ‰ª•‰πüÂèØ‰ª•Áî®‚ÄúZ‚ÄùÊù•ÊõøÊç¢‚ÄúZa‚ÄùÔºâÔºåÂæóÂà∞ÊõøÊç¢Ë°®‰ª•ÂèäÊï∞ÊçÆ\n",
    "Z <- aa, Y <- Za, YbdYbac. Êàë‰ª¨ÂÜçÊ¨°ÊõøÊç¢ÊúÄÂ∏∏Âá∫Áé∞ÁöÑÂ≠óËäÇÂØπÂæóÂà∞ÔºöZ <- aa, Y <- Za, X <- Yb. XdXac Áî±‰∫é‰∏çÂÜçÊúâÈáçÂ§çÂá∫Áé∞ÁöÑÂ≠óËäÇÂØπÔºåÊâÄ‰ª•Ëøô‰∏™Êï∞ÊçÆ‰∏çËÉΩÂÜçË¢´Ëøõ‰∏ÄÊ≠•ÂéãÁº©„ÄÇËß£ÂéãÁöÑÊó∂ÂÄôÔºåÂ∞±ÊòØÊåâÁÖßÁõ∏ÂèçÁöÑÈ°∫Â∫èÊâßË°åÊõøÊç¢ËøáÁ®ã„ÄÇ\n",
    "\"\"\"\n",
    "doc = nlp(text)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6033d4-5b9c-4d1d-bd66-8a97a22c0fa5",
   "metadata": {},
   "source": [
    "## 4. LLMs tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11028936-e87f-4208-a65a-6f91eb2870ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::tiktoken -y --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1465086a-ca17-4aac-b649-237d9dba5cff",
   "metadata": {},
   "source": [
    "### 4.1 Subword tokenization: BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe86061-f066-45e3-a2ff-b672a079808a",
   "metadata": {},
   "source": [
    "### 4.4 Huggingface tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6b77b-fd37-4bfd-ba5c-537896f0a51d",
   "metadata": {},
   "source": [
    "PreTrainedTokenizer, PreTrainedTokenizerBase, AutoTokenizer\n",
    "\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils.py\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_fast.py\n",
    "- Check tokenizers at https://github.com/huggingface/transformers/blob/main/setup.py\n",
    "- If you want to train a tokenizer by yourself, then go to: https://github.com/huggingface/tokenizers\n",
    "- A fast BPE tokenizer is also at: https://github.com/openai/tiktoken\n",
    "- An implementation of sentencepiece is at: https://github.com/google/sentencepiece\n",
    "- There are 3 most common methods for tokenization: https://github.com/huggingface/tokenizers/tree/main/tokenizers/src/models\n",
    "- - BPE: https://aclanthology.org/P16-1162.pdf\n",
    "  - Unigram: https://arxiv.org/pdf/1804.10959\n",
    "  - WordPiece https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec32b0-3e36-4668-94a9-827944dc7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2Tokenizer, Qwen2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb35d18-f1f8-4777-9f9b-2424899234db",
   "metadata": {},
   "source": [
    "## 5. Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93ac5d-0ea5-45b2-9a56-5ece2dcfd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define minimum edit distance algorithm via dynamic programming\n",
    "def minimum_edit_distance(source, target):\n",
    "    n = len(source)\n",
    "    m = len(target)\n",
    "    d_mat = np.zeros((n + 1, m + 1))\n",
    "    for i in range(1, n + 1):\n",
    "        d_mat[i, 0] = i\n",
    "    for j in range(1, m + 1):\n",
    "        d_mat[0, j] = j\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            sub = 0 if source[i - 1] == target[j - 1] else 2\n",
    "            del_ = d_mat[i - 1][j] + 1\n",
    "            ins_ = d_mat[i][j - 1] + 1\n",
    "            d_mat[i][j] = min(del_, ins_, d_mat[i - 1][j - 1] + sub)\n",
    "    trace, align_source, align_target = backtrack_alignment(source, target, d_mat)\n",
    "    return d_mat[n, m], trace, align_source, align_target\n",
    "\n",
    "def backtrack_alignment(source, target, d_mat):\n",
    "    align_source, align_target = [], []\n",
    "    i, j = len(source), len(target)\n",
    "    back_trace = [[i, j]]\n",
    "\n",
    "    while (i, j) != (0, 0):\n",
    "        # boundary cases first (avoid negative indexing)\n",
    "        if i == 0:\n",
    "            back_trace.append([i, j - 1])\n",
    "            align_source = [\"*\"] + align_source\n",
    "            align_target = [target[j - 1]] + align_target\n",
    "            j -= 1\n",
    "            continue\n",
    "        if j == 0:\n",
    "            back_trace.append([i - 1, j])\n",
    "            align_source = [source[i - 1]] + align_source\n",
    "            align_target = [\"*\"] + align_target\n",
    "            i -= 1\n",
    "            continue\n",
    "\n",
    "        sub_cost = 0 if source[i - 1] == target[j - 1] else 2\n",
    "\n",
    "        # prefer substitution/match when optimal (your tie-break rule)\n",
    "        if d_mat[i, j] == d_mat[i - 1, j - 1] + sub_cost:\n",
    "            back_trace.append([i - 1, j - 1])\n",
    "            align_source = [source[i - 1]] + align_source\n",
    "            align_target = [target[j - 1]] + align_target\n",
    "            i, j = i - 1, j - 1\n",
    "\n",
    "        # deletion\n",
    "        elif d_mat[i, j] == d_mat[i - 1, j] + 1:\n",
    "            back_trace.append([i - 1, j])\n",
    "            align_source = [source[i - 1]] + align_source\n",
    "            align_target = [\"*\"] + align_target\n",
    "            i -= 1\n",
    "\n",
    "        # insertion\n",
    "        else:\n",
    "            back_trace.append([i, j - 1])\n",
    "            align_source = [\"*\"] + align_source\n",
    "            align_target = [target[j - 1]] + align_target\n",
    "            j -= 1\n",
    "\n",
    "    return back_trace, align_source, align_target\n",
    "\n",
    "# test the minimum edit distance\n",
    "def test_med(source, target):\n",
    "    med, trace, align_source, align_target = minimum_edit_distance(source, target)\n",
    "    print(f\"input source: {source} and target: {target}\")\n",
    "    print(f\"med: {med}\")\n",
    "    print(f\"trace: {trace}\")\n",
    "    print(f\"aligned source: {align_source}\")\n",
    "    print(f\"aligned target: {align_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b108a5-bd4a-497f-9a6b-e8fd35bce6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_med(source=\"INTENTION\", target=\"EXECUTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454ca67-0981-4464-9f7c-11703060b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_med(source=\"AGGCTATCACCTGACCTCCAGGCCGATGCCC\", target=\"TAGCTATCACGACCGCGGTCGATTTGCCCGAC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
